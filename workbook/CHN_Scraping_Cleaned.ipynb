{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted from Current_season_scrape notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('goalie_stats',), ('line_chart',), ('advanced_metrics',), ('game_details',), ('player_stats',), ('linescore',), ('penalty_summary',), ('scoring_summary',), ('player_stats_ytd',), ('master_roster',)]\n",
      "[(1006,)]\n"
     ]
    }
   ],
   "source": [
    "## Dependencies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "## Global Variables\n",
    "### pasth to schedule results table on colleg ehockey news\n",
    "current_year_url = 'https://www.collegehockeynews.com/schedules/?season=20232024'\n",
    "\n",
    "# Path to database file - using os.path.join to make it OS agnostic\n",
    "# data\\db\\Current_YTD_Stats.db\n",
    "db_path = os.path.join('..', 'data', 'db', 'END_FEB_YTD_Stats.db') \n",
    "\n",
    "# Path to log file\n",
    "log_path = '../TEMP/SCRAPE_Current_YTD_Stats.log'\n",
    "\n",
    "### Check DB path and tables\n",
    "if not os.path.exists(db_path):\n",
    "    raise ValueError(f'Database file not found at {db_path}')\n",
    "\n",
    "# Print table names in database\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cursor.fetchall())\n",
    "\n",
    "# Count and print the number of Games in the database (unique game_id's)\n",
    "cursor.execute(\"SELECT COUNT(DISTINCT game_id) FROM game_details;\")\n",
    "print(cursor.fetchall())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Schedule / Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total games played so far: 1186\n"
     ]
    }
   ],
   "source": [
    "# Parses the season schedule page and returns a list of list with all games with links to box scores and metrics\n",
    "def parse_current_season(url):\n",
    "        # Initialize variables\n",
    "    current_date = None\n",
    "    current_conference = None\n",
    "    game_notes = None\n",
    "\n",
    "    # Initialize an empty list to hold the data\n",
    "    data = []\n",
    "\n",
    "    # Parse the page with BeautifulSoup\n",
    "    # Get the page with requests\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # select the table or tables\n",
    "    tables = soup.find_all('table')\n",
    "\n",
    "    rows = soup.find_all('tr')\n",
    "\n",
    "    # Loop through each row to find relevant information\n",
    "    for row in rows:\n",
    "        # Check for date row\n",
    "        if row.get('class') == ['stats-section']:\n",
    "            current_date = row.find('td').text.strip()\n",
    "        # Check for conference row\n",
    "        elif row.get('class') == ['sked-header']:\n",
    "            current_conference = row.find('td').text.strip()\n",
    "        # Check for game notes\n",
    "        elif len(row.find_all('td')) == 2:\n",
    "            game_notes = row.find_all('td')[1].text.strip()\n",
    "        # Process rows with game data\n",
    "        elif row.get('valign') == 'top':\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 9:\n",
    "                home_team = cells[0].text.strip()\n",
    "                # Remove any hyphens from the team name\n",
    "                home_team = home_team.replace('-', ' ')\n",
    "                home_team_link = cells[0].find('a')['href'] if cells[0].find('a') else None\n",
    "                home_score = cells[1].text.strip()\n",
    "                away_team = cells[3].text.strip()\n",
    "                away_team_link = cells[3].find('a')['href'] if cells[3].find('a') else None\n",
    "                away_score = cells[4].text.strip()\n",
    "                ot = cells[5].text.strip()\n",
    "                box_link = cells[7].find('a')['href'] if cells[7].find('a') else None\n",
    "                metrics_link = cells[8].find('a')['href'] if cells[8].find('a') else None\n",
    "                # Capture Game Notes\n",
    "                game_notes_cell = cells[-1].find('small')\n",
    "                game_notes = game_notes_cell.text.strip() if game_notes_cell else None\n",
    "\n",
    "                # Append data to the list\n",
    "                data.append([current_date, current_conference, game_notes, home_team, home_team_link, home_score, away_team, away_team_link, away_score, ot, box_link, metrics_link])\n",
    "                game_notes = None  # Reset game notes for the next row\n",
    "    return data\n",
    "\n",
    "## call the function\n",
    "data = parse_current_season(current_year_url)\n",
    "\n",
    "\n",
    "# Create a dataframe from the list\n",
    "\n",
    "columns = ['Date', 'Conference', 'Game_Notes', 'Home_Team', 'Home_Team_Link', 'Home_Score', 'Away_Team', 'Away_Team_Link', 'Away_Score', 'OT', 'Box_Link', 'Metrics_Link']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "            \n",
    "## Extract the day of the week from the date and save in new column\n",
    "df['Day'] = pd.to_datetime(df['Date']).dt.day_name()\n",
    "# remove day of the week from date\n",
    "# format data column as YYYY-MM-DD\n",
    "df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "### Create a new column for the game ID\n",
    "## Game ID will be a combination of the date and abbreviated team names\n",
    "\n",
    "# Function to abbreviate the team names\n",
    "for row in df.itertuples():\n",
    "    home_team = row.Home_Team\n",
    "    away_team = row.Away_Team\n",
    "    home_team_abbr = home_team.split(' ')[-1]\n",
    "    away_team_abbr = away_team.split(' ')[-1]\n",
    "    # Remove any hyphens from the team name if there are any\n",
    "    home_team_abbr = home_team_abbr.replace('-', ' ')\n",
    "    away_team_abbr = away_team_abbr.replace('-', ' ')\n",
    "    game_id = f'{row.Date}-{home_team_abbr}-{away_team_abbr}'\n",
    "    df.loc[row.Index, 'Game_ID'] = game_id\n",
    "\n",
    "# Create a new column for the game ID\n",
    "df['Game_ID'] = df['Game_ID'].str.replace(',', '')\n",
    "\n",
    "# Remove any hyphens from the team names if any\n",
    "df['Home_Team'] = df['Home_Team'].str.replace('-', ' ')\n",
    "df['Away_Team'] = df['Away_Team'].str.replace('-', ' ')\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['Game_ID'] = df.apply(lambda row: f'{row.Date}-{row.Home_Team}-{row.Away_Team}', axis=1)\n",
    "\n",
    "## Filter out games that have not been played yet\n",
    "df = df[df['Home_Score'] != '']\n",
    "\n",
    "# Replace Nan values in metrics column with empty string\n",
    "df['Metrics_Link'] = df['Metrics_Link'].fillna('')\n",
    "\n",
    "# Print the amount of Total Games Played games\n",
    "print(f'Total games played so far: {len(df) }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of games already scraped: 1006\n",
      "Number of games to be scraped: 180\n"
     ]
    }
   ],
   "source": [
    "### Filter to only games that have not been scraped yet\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Query the database for the games that have already been scraped\n",
    "query = 'SELECT DISTINCT Game_ID FROM game_details'\n",
    "scraped_games = pd.read_sql(query, conn)\n",
    "\n",
    "# Print length of scraped games\n",
    "print(f'Number of games already scraped: {len(scraped_games)}')\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Filter the DataFrame to only games that have not been scraped\n",
    "df = df[~df['Game_ID'].isin(scraped_games['Game_ID'])]\n",
    "\n",
    "# Reset the index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Check the first few rows of the DataFrame\n",
    "# df.tail()\n",
    "\n",
    "# df.info()\n",
    "\n",
    "# RENAME THE DATAFRAME TO FEED INTO SCRAPER\n",
    "unscraped_games = df\n",
    "\n",
    "# Print the amount of games to be scraped\n",
    "print(f'Number of games to be scraped: {len(unscraped_games)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions - Unchanged from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for parsing the box score and metrics pages\n",
    "\n",
    "# Initialize logging for Error and Warning messages\n",
    "logging.basicConfig(filename='../TEMP/current_scrape.log', level=logging.INFO)\n",
    "\n",
    "#### PARSE PLAYER STATS TABLE ####\n",
    "def parse_player_summary(html_content):\n",
    "    # Initialize BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find the playersums div\n",
    "    playersums_div = soup.find('div', id='playersums')\n",
    "    if playersums_div is None:\n",
    "        return \"Player summaries div not found\"\n",
    "\n",
    "    # Initialize list to store player stats\n",
    "    player_stats = []\n",
    "\n",
    "    # Loop through each playersum div\n",
    "    for player_sum in playersums_div.find_all('div', class_='playersum'):\n",
    "        team = player_sum.find('td').text.strip()\n",
    "        \n",
    "        # Loop through table rows\n",
    "        for row in player_sum.find_all('tr'):\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 1:\n",
    "                player = cols[0].text.strip()\n",
    "                goals = cols[1].text.strip()\n",
    "                assists = cols[2].text.strip()\n",
    "                points = cols[3].text.strip()\n",
    "                plus_minus = cols[4].text.strip()\n",
    "                shots = cols[5].text.strip()\n",
    "                pim = cols[6].text.strip()\n",
    "                fowl = cols[7].text.strip() if len(cols) > 7 else None\n",
    "                \n",
    "                fow, fol = None, None\n",
    "                win_percentage = None\n",
    "                \n",
    "                \n",
    "\n",
    "                try:\n",
    "                    if fowl and '‑' in fowl:  # Checking if it contains a hyphen\n",
    "                        fow, fol = map(int, fowl.split('‑'))\n",
    "                        total_fo = fow + fol\n",
    "                        win_percentage = (fow / total_fo) * 100 if total_fo > 0 else 0\n",
    "                except ValueError:\n",
    "                    fow, fol, win_percentage = None, None, None\n",
    "\n",
    "                \n",
    "\n",
    "                \n",
    "                player_stat = {\n",
    "                    'Team': team,\n",
    "                    'Player': player,\n",
    "                    'G': goals,\n",
    "                    'A': assists,\n",
    "                    'Pt.': points,\n",
    "                    '+/-': plus_minus,\n",
    "                    'Sh': shots,\n",
    "                    'PIM': pim,\n",
    "                    'FOW': fow,\n",
    "                    'FOL': fol,\n",
    "                    'FO%': win_percentage\n",
    "                }\n",
    "                player_stats.append(player_stat)\n",
    "\n",
    "    return pd.DataFrame(player_stats)\n",
    "\n",
    "\n",
    "############# PARSEING SCORING SUMMARY WITH BS4\n",
    "def parse_scoring_summary(html_content):\n",
    "    # Initialize BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the scoring div and table\n",
    "    scoring_div = soup.find('div', id='scoring')\n",
    "    if scoring_div is None:\n",
    "        logging.error(\"Scoring div not found\")\n",
    "        return None\n",
    "\n",
    "    scoring_table = scoring_div.find('table')\n",
    "    if scoring_table is None:\n",
    "        logging.error(\"Scoring table not found within the scoring div\")\n",
    "        return None\n",
    "\n",
    "    # Initialize list to store scoring events\n",
    "    scoring_events = []\n",
    "    period = None\n",
    "\n",
    "    # Loop through table rows\n",
    "    for row in scoring_table.find_all('tr'):\n",
    "        if 'stats-section' in row.get('class', []):\n",
    "            td = row.find('td')\n",
    "            if td:\n",
    "                period = td.text.strip()\n",
    "            else:\n",
    "                logging.warning(\"Period name not found in 'stats-section' row\")\n",
    "                period = \"Unknown\"\n",
    "        else:\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 1:\n",
    "                try:\n",
    "                    team = cols[0].text.strip()\n",
    "                    pp = cols[1].text.strip()\n",
    "\n",
    "                    player_data = cols[3].text.strip()\n",
    "                    match = re.match(r\"(.+)\\s\\((\\d+)\\)\", player_data)\n",
    "                    player = match.group(1) if match else player_data\n",
    "                    goals = int(match.group(2)) if match else None\n",
    "\n",
    "                    assist_data_raw = cols[4].text.strip()\n",
    "                    assist_data = assist_data_raw.split(\", \") if assist_data_raw else []\n",
    "                    assist1 = assist_data[0] if len(assist_data) > 0 else None\n",
    "                    assist2 = assist_data[1] if len(assist_data) > 1 else None\n",
    "\n",
    "                    time = cols[5].text.strip()\n",
    "\n",
    "                    scoring_event = {\n",
    "                        'Period': period,\n",
    "                        'Team': team,\n",
    "                        'PP': pp,\n",
    "                        'Player': player,\n",
    "                        'Player_Goals': goals,\n",
    "                        'Assist1': assist1,\n",
    "                        'Assist2': assist2,\n",
    "                        'Time': time\n",
    "                    }\n",
    "                    scoring_events.append(scoring_event)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"An error occurred while parsing a scoring event row: {e}\")\n",
    "            else:\n",
    "                logging.warning(f\"Insufficient columns in scoring row: {len(cols)}\")\n",
    "\n",
    "    return pd.DataFrame(scoring_events)\n",
    "\n",
    "\n",
    "def parse_game_details(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    meta_div = soup.find('div', {'id': 'meta'})\n",
    "    if meta_div is None:\n",
    "        logging.error(\"Meta div not found\")\n",
    "        return None\n",
    "    \n",
    "    game_details_div = meta_div.find_all('div')[-1]\n",
    "    if game_details_div is None:\n",
    "        logging.error(\"Game details div not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        date_str = game_details_div.h4.string\n",
    "        day_of_week, date = date_str.split(\", \", 1)\n",
    "        \n",
    "        p_elements = game_details_div.find_all('p')\n",
    "        \n",
    "        # Extract conference and location details\n",
    "        for p in p_elements:\n",
    "            if \"Game\" in p.text:  # e.g., \"Big Ten Game\"\n",
    "                details_strs = p.get_text(separator='|').split('|')\n",
    "                conference = details_strs[0]\n",
    "                location = details_strs[-1].split('at ')[-1]\n",
    "                break\n",
    "        else:  # Defaults if not found\n",
    "            conference, location = None, None\n",
    "        \n",
    "        # Extract referees and assistant referees details\n",
    "        for p in p_elements:\n",
    "            if \"Referees\" in p.text:\n",
    "                refs_str = p.strong.next_sibling if p.strong else None\n",
    "                asst_refs_str = p.find_all('strong')[1].next_sibling if len(p.find_all('strong')) > 1 else None\n",
    "                break\n",
    "        else:  # Defaults if not found\n",
    "            refs_str, asst_refs_str = None, None\n",
    "        \n",
    "        refs = refs_str.split(', ') if refs_str else []\n",
    "        asst_refs = asst_refs_str.split(', ') if asst_refs_str else []\n",
    "        refs = [re.sub(r'[^a-zA-Z ]+', '', ref).strip() for ref in refs]\n",
    "        asst_refs = [re.sub(r'[^a-zA-Z ]+', '', ref).strip() for ref in asst_refs]\n",
    "        \n",
    "        # Extract attendance details using regex for better accuracy\n",
    "        attendance_pattern = r\"Attendance:\\s?(\\d+[\\d,]*)\"\n",
    "        attendance_match = re.search(attendance_pattern, html_content)\n",
    "        attendance = int(attendance_match.group(1).replace(',', '')) if attendance_match else None\n",
    "        \n",
    "        # Extract game details (like shootout results)\n",
    "        details = None\n",
    "        for p in p_elements:\n",
    "            if \"shootout\" in p.text:\n",
    "                details = p.text\n",
    "                break\n",
    "        \n",
    "        # Clean details if present\n",
    "        if details and '\\n' in details:\n",
    "            details = details.replace('\\n', '').strip()\n",
    "        if details and '\\t' in details:\n",
    "            details = re.sub('\\t', ' ', details)\n",
    "        \n",
    "        game_details = {\n",
    "            'Day': day_of_week,\n",
    "            'Date': date,\n",
    "            'Conference': conference,\n",
    "            'Details': details,\n",
    "            'Location': location,\n",
    "            'Ref1': refs[0] if refs else None,\n",
    "            'Ref2': refs[1] if len(refs) > 1 else None,\n",
    "            'Asst_Ref1': asst_refs[0] if asst_refs else None,\n",
    "            'Asst_Ref2': asst_refs[1] if len(asst_refs) > 1 else None,\n",
    "            'Attendance': attendance\n",
    "        }\n",
    "        \n",
    "        game_details_df = pd.DataFrame([game_details])\n",
    "        return game_details_df\n",
    "\n",
    "    except (AttributeError, IndexError, ValueError) as e:\n",
    "        logging.error(f\"Error while parsing game details: {e}\")\n",
    "        return None\n",
    "\n",
    "### Get the Linescore Elements - Score, shots, ect by period####\n",
    "### UPDATED \n",
    "\n",
    "def parse_linescore(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    linescore_data = []\n",
    "    \n",
    "    # Parsing the Goals table\n",
    "    goals_table = soup.select_one(\"#goals table\")\n",
    "    if goals_table is None:\n",
    "        logging.error(\"Goals table not found\")\n",
    "        return None\n",
    "    \n",
    "    rows = goals_table.select('tbody tr')\n",
    "    if not rows:\n",
    "        logging.warning(\"No rows found in Goals table\")\n",
    "        return None\n",
    "    \n",
    "    for row in rows:\n",
    "        team_data = {}\n",
    "        td = row.select_one('td')\n",
    "        if td:\n",
    "            team_data['Team'] = td.text\n",
    "        else:\n",
    "            logging.warning(\"Team name not found in Goals table\")\n",
    "            continue\n",
    "\n",
    "        goals = row.select('td')[1:]\n",
    "        for i, goal in enumerate(goals):\n",
    "            period = i + 1\n",
    "            column_name = f'goals{period}' if i < len(goals) - 1 else 'goalsT'\n",
    "            team_data[column_name] = int(goal.text)\n",
    "        \n",
    "        linescore_data.append(team_data)\n",
    "    \n",
    "\n",
    "    # Parsing the Shots table\n",
    "    shots_table = soup.select_one(\"#shots table\")\n",
    "    if shots_table is None:\n",
    "        logging.error(\"Shots table not found\")\n",
    "        return None\n",
    "\n",
    "    rows = shots_table.select('tbody tr')\n",
    "    if not rows:\n",
    "        logging.warning(\"No rows found in Shots table\")\n",
    "        return None\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        shots = row.select('td')[1:]\n",
    "        if not shots:\n",
    "            logging.warning(f\"No shot data found for row {i+1} in Shots table\")\n",
    "            continue\n",
    "\n",
    "        for j, shot in enumerate(shots):\n",
    "            period = j + 1\n",
    "            column_name = f'shots{period}' if j < len(shots) - 1 else 'shotsT'\n",
    "            try:\n",
    "                linescore_data[i][column_name] = int(shot.text.strip())\n",
    "            except ValueError:\n",
    "                logging.warning(f\"Could not convert shot data to integer for row {i+1}, column {j+1}\")\n",
    "                linescore_data[i][column_name] = None\n",
    "\n",
    "    # Parsing the PP table\n",
    "    pp_table = soup.select_one(\"#pp table\")\n",
    "    if pp_table is None:\n",
    "        logging.error(\"PP table not found\")\n",
    "        return None\n",
    "\n",
    "    rows = pp_table.select('tbody tr')\n",
    "    if not rows:\n",
    "        logging.warning(\"No rows found in PP table\")\n",
    "        return None\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        try:\n",
    "            pen_pim = row.select('td')[1].text.split('‑')\n",
    "            linescore_data[i]['Pen'] = int(pen_pim[0])\n",
    "            linescore_data[i]['PIM'] = int(pen_pim[1])\n",
    "\n",
    "            ppg_ppo = row.select('td')[2].text.split('‑')\n",
    "            linescore_data[i]['PPG'] = int(ppg_ppo[0])\n",
    "            linescore_data[i]['PPO'] = int(ppg_ppo[1])\n",
    "\n",
    "            fow_fol = row.select('td')[3].text.split('‑')\n",
    "            linescore_data[i]['FOW'] = int(fow_fol[0])\n",
    "            linescore_data[i]['FOL'] = int(fow_fol[1])\n",
    "            linescore_data[i]['FOW%'] = (linescore_data[i]['FOW'] / (linescore_data[i]['FOW'] + linescore_data[i]['FOL'])) * 100\n",
    "\n",
    "        except (ValueError, IndexError) as e:\n",
    "            logging.warning(f\"Could not process PP data for row {i+1}. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Convert to DataFrame early\n",
    "    df = pd.DataFrame(linescore_data)\n",
    "\n",
    "    # Ensure all columns exist\n",
    "    expected_goals_columns = [f'goals{i}' for i in range(1, 5)] + ['goalsT']\n",
    "    expected_shots_columns = [f'shots{i}' for i in range(1, 5)] + ['shotsT']\n",
    "\n",
    "    for col in expected_goals_columns + expected_shots_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    # Return the final DataFrame\n",
    "    return df\n",
    "\n",
    "def parse_box_score(box_score_html):\n",
    "    # Initialize DataFrames to None\n",
    "    scoring_summary = penalty_summary = goalie_stats = player_stats = line_chart = linescore = game_details = None\n",
    "    \n",
    "    try:\n",
    "        scoring_summary = parse_scoring_summary(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_scoring_summary: {e}\")\n",
    "    \n",
    "    try:\n",
    "        penalty_summary = parse_penalty_summary(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_penalty_summary: {e}\")\n",
    "    \n",
    "    try:\n",
    "        goalie_stats = parse_goalie_stats(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_goalie_stats: {e}\")\n",
    "    \n",
    "    try:\n",
    "        player_stats = parse_player_summary(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_player_summary: {e}\")\n",
    "    \n",
    "    try:\n",
    "        line_chart = parse_line_chart(box_score_html)\n",
    "        if line_chart.empty:\n",
    "            logging.info(\"Line chart is empty. Skipping the insert for this game.\")\n",
    "        else:\n",
    "            logging.info(f\"Line chart DataFrame structure: {line_chart.dtypes}\")\n",
    "\n",
    "        # Insert into database (make sure this part works as expected)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in parse_line_chart: {e}\")\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        linescore = parse_linescore(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_linescore: {e}\")\n",
    "    \n",
    "    try:\n",
    "        game_details = parse_game_details(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_game_details: {e}\")\n",
    "    \n",
    "    # Combine DataFrames into a list\n",
    "    all_dfs = [game_details, scoring_summary, penalty_summary, goalie_stats, player_stats, line_chart, linescore]\n",
    "    \n",
    "    return all_dfs\n",
    "\n",
    "#### PARSE THE ADVANCED TEAM METRICS TABLES ####\n",
    "### NEW - IT WORKS IN THE NOTEBOOK BUT NOT IN THE FUNCTION\n",
    "### RETURNS WHOLE ADVANCED METRICS AS SINGLE TABLE\n",
    "####################################\n",
    "def parse_new_advanced_metrics(html_content):\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all tables with advanced metrics\n",
    "    tables = soup.find_all('table', {'class': 'sortable metrics'})\n",
    "    \n",
    "    # List to store all parsed data\n",
    "    all_data = []\n",
    "    \n",
    "    for table in tables:\n",
    "        # Extract team name from the table header\n",
    "        team_name = table.find('td').text.strip()\n",
    "        \n",
    "        # Extract headers (skipping the Player header)\n",
    "        headers = [header.text for header in table.find_all('th')][1:]\n",
    "        \n",
    "        # Prepare final column headers\n",
    "        col_names = ['Team', 'Player']\n",
    "        for header in headers:\n",
    "            col_names.append(header)\n",
    "        \n",
    "        # Extract player data\n",
    "        rows = table.find_all('tr')[2:]  # skipping the two header rows\n",
    "        for row in rows:\n",
    "            player_data = [team_name]  # start with team name\n",
    "            cells = row.find_all('td')\n",
    "            player_data.append(cells[0].text.strip())  # player name\n",
    "            for cell in cells[1:]:\n",
    "                player_data.append(cell.text.strip())\n",
    "            all_data.append(player_data)\n",
    "    \n",
    "    # Convert the list of data to a DataFrame\n",
    "    df = pd.DataFrame(all_data, columns=col_names)\n",
    "    return df\n",
    "\n",
    "### May Be Outdated unnecessary\n",
    "def parse_advanced_metrics_tables(html_content):\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all tables with advanced metrics\n",
    "    tables = soup.find_all('table', {'class': 'sortable metrics'})\n",
    "    \n",
    "    # List to store all parsed data\n",
    "    all_data = []\n",
    "    \n",
    "    for table in tables:\n",
    "        # Extract team name from the table header\n",
    "        team_name = table.find('td').text.strip()\n",
    "        \n",
    "        # Extract headers (skipping the Player header)\n",
    "        headers = [header.text for header in table.find_all('th')][1:]\n",
    "        \n",
    "        # Prepare final column headers\n",
    "        col_names = ['Team', 'Player']\n",
    "        for header in headers:\n",
    "            col_names.append(header)\n",
    "        \n",
    "        # Extract player data\n",
    "        rows = table.find_all('tr')[2:]  # skipping the two header rows\n",
    "        for row in rows:\n",
    "            player_data = [team_name]  # start with team name\n",
    "            cells = row.find_all('td')\n",
    "            player_data.append(cells[0].text.strip())  # player name\n",
    "            for cell in cells[1:]:\n",
    "                player_data.append(cell.text.strip())\n",
    "            all_data.append(player_data)\n",
    "    \n",
    "    # Convert the list of data to a DataFrame\n",
    "    df = pd.DataFrame(all_data, columns=col_names)\n",
    "    return df\n",
    "\n",
    "# Complete code for parsing the line chart information with specific positions for forwards and defensemen.\n",
    "def parse_line_chart(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    line_chart_div = soup.find('div', id='linechart')\n",
    "\n",
    "    if line_chart_div is None:\n",
    "        logging.error(\"Line chart div not found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    line_data = []\n",
    "\n",
    "    for team_div in line_chart_div.find_all('div', recursive=False):\n",
    "        h3 = team_div.find('h3')\n",
    "        if h3 is None:\n",
    "            logging.warning(\"Team name not found\")\n",
    "            continue\n",
    "        \n",
    "        team_name = h3.text.strip()\n",
    "        \n",
    "        for line_type_div in team_div.find_all('div', recursive=False):\n",
    "            line_type = line_type_div.get('class')[0] if line_type_div.get('class') else None\n",
    "            if line_type is None:\n",
    "                logging.warning(\"Line type not found\")\n",
    "                continue\n",
    "            \n",
    "            if line_type == 'f':\n",
    "                position_types = ['Left Wing', 'Center', 'Right Wing']\n",
    "            elif line_type == 'd':\n",
    "                position_types = ['Left D', 'Right D']\n",
    "            elif line_type == 'x':\n",
    "                position_types = ['Extra']\n",
    "            elif line_type == 'g':\n",
    "                position_types = ['Goalie']\n",
    "                goalie_count = 1  # Initialize goalie count\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            players = line_type_div.find_all('div')\n",
    "            if not players:\n",
    "                logging.warning(f\"No players found for {team_name} in {line_type}\")\n",
    "                continue\n",
    "            \n",
    "            for i, player in enumerate(players):\n",
    "                player_name = player.text.strip()\n",
    "                if line_type == 'x':\n",
    "                    player_name = player_name.split(' ')[0]\n",
    "                if line_type == 'g':\n",
    "                    line_number = f\"Goalie {goalie_count}\"\n",
    "                    goalie_count += 1\n",
    "                else:\n",
    "                    line_number = i // len(position_types) + 1\n",
    "\n",
    "                position = position_types[i % len(position_types)]\n",
    "                line_data.append({\n",
    "                    'Team': team_name,\n",
    "                    'Line': line_number,\n",
    "                    'Position': position,\n",
    "                    'Player': player_name\n",
    "                })\n",
    "\n",
    "    if not line_data:\n",
    "        logging.error(\"No line data was collected\")\n",
    "\n",
    "    df = pd.DataFrame(line_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "############# PARSEING PENALTY SUMMARY WITH BS4\n",
    "\n",
    "def parse_penalty_summary(html_content):\n",
    "    # Initialize BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the penalties div and table\n",
    "    penalties_div = soup.find('div', id='penalties')\n",
    "    if penalties_div is None:\n",
    "        logging.error(\"Penalties div not found\")\n",
    "        return None\n",
    "\n",
    "    penalties_table = penalties_div.find('table')\n",
    "    if penalties_table is None:\n",
    "        logging.error(\"Penalties table not found within the penalties div\")\n",
    "        return None\n",
    "\n",
    "    # Initialize list to store penalty events\n",
    "    penalty_events = []\n",
    "    period = None\n",
    "\n",
    "    # Loop through table rows\n",
    "    for row in penalties_table.find_all('tr'):\n",
    "        if 'stats-section' in row.get('class', []):\n",
    "            td = row.find('td')\n",
    "            if td:\n",
    "                period = td.text.strip()\n",
    "            else:\n",
    "                logging.warning(\"Period name not found in 'stats-section' row\")\n",
    "                period = \"Unknown\"\n",
    "        else:\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 1:\n",
    "                team = cols[0].text.strip()\n",
    "                player = cols[1].text.strip()\n",
    "                pen_length = cols[2].text.strip()\n",
    "                penalty_type = cols[3].text.strip()\n",
    "                time = cols[4].text.strip()\n",
    "\n",
    "                penalty_event = {\n",
    "                    'Period': period,\n",
    "                    'Team': team,\n",
    "                    'Player': player,\n",
    "                    'Pen_Length': pen_length,\n",
    "                    'Penalty_Type': penalty_type,\n",
    "                    'Time': time\n",
    "                }\n",
    "                penalty_events.append(penalty_event)\n",
    "\n",
    "    return pd.DataFrame(penalty_events)\n",
    "\n",
    "\n",
    "############# GOALIE SUMMARY WITH BS4\n",
    "def parse_goalie_stats(html_content):\n",
    "    # Initialize BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the goalies div and table\n",
    "    goalies_div = soup.find('div', id='goalies')\n",
    "    if goalies_div is None:\n",
    "        logging.error(\"Goalies div not found\")\n",
    "        return None\n",
    "\n",
    "    goalies_table = goalies_div.find('table')\n",
    "    if goalies_table is None:\n",
    "        logging.error(\"Goalies table not found within the goalies div\")\n",
    "        return None\n",
    "\n",
    "    # Initialize list to store goalie stats\n",
    "    goalie_stats = []\n",
    "    team = None\n",
    "\n",
    "    # Loop through table rows\n",
    "    for row in goalies_table.find_all('tr'):\n",
    "        if 'stats-header' in row.get('class', []):\n",
    "            td = row.find('td')\n",
    "            if td:\n",
    "                team = td.text.strip()\n",
    "            else:\n",
    "                logging.warning(\"Team name not found in 'stats-header' row\")\n",
    "                team = \"Unknown\"\n",
    "        else:\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 1:\n",
    "                goalie = cols[0].text.strip()\n",
    "                sv = cols[1].text.strip()\n",
    "                ga = cols[2].text.strip()\n",
    "                minutes = cols[3].text.strip()\n",
    "\n",
    "                goalie_stat = {\n",
    "                    'Team': team,\n",
    "                    'Goalie': goalie,\n",
    "                    'SV': sv,\n",
    "                    'GA': ga,\n",
    "                    'Minutes': minutes\n",
    "                }\n",
    "                goalie_stats.append(goalie_stat)\n",
    "\n",
    "    return pd.DataFrame(goalie_stats)\n",
    "\n",
    "\n",
    "\n",
    "def rename_duplicate_columns(df):\n",
    "    cols = pd.Series(df.columns)\n",
    "    for dup in df.columns[df.columns.duplicated()].unique(): \n",
    "        cols[df.columns.get_loc(dup)] = [f\"{dup}_{i}\" if i != 0 else dup for i in range(df.columns.get_loc(dup).sum())]\n",
    "    df.columns = cols\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the unscrapped games through the all of the individual ans store the results in the data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to call all of the game scraping sub functions\n",
    "\n",
    "def scrape_game(game_id, box_link, metrics_link):\n",
    "    # Initialize variables\n",
    "    box_score_html = None\n",
    "    metrics_html = None\n",
    "\n",
    "    # Get the box score page\n",
    "    if box_link:\n",
    "        try:\n",
    "            box_score_response = requests.get(box_link)\n",
    "            if box_score_response.status_code == 200 and box_score_response.text:\n",
    "                box_score_html = box_score_response.text\n",
    "            else:\n",
    "                logging.error(f\"Box score page returned status code {box_score_response.status_code} or is empty for game {game_id}\")\n",
    "                # Optionally, continue to the next game if the box score page is not available\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to fetch box score page for game {game_id}: {e}\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "    # Get the metrics page\n",
    "    if metrics_link:\n",
    "        try:\n",
    "            metrics_response = requests.get(metrics_link)\n",
    "            if metrics_response.status_code == 200:\n",
    "                metrics_html = metrics_response.text\n",
    "            else:\n",
    "                logging.error(f\"Metrics page returned status code {metrics_response.status_code}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to fetch advanced metrics page for game {game_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Parse the box score page\n",
    "    all_dfs = parse_box_score(box_score_html)\n",
    "    if all_dfs:\n",
    "        game_details, scoring_summary, penalty_summary, goalie_stats, player_stats, line_chart, linescore = all_dfs\n",
    "    else:\n",
    "        game_details, scoring_summary, penalty_summary, goalie_stats, player_stats, line_chart, linescore = None, None, None, None, None, None, None\n",
    "\n",
    "    # Rename duplicate columns\n",
    "    if scoring_summary is not None:\n",
    "        scoring_summary = rename_duplicate_columns(scoring_summary)\n",
    "    if penalty_summary is not None:\n",
    "        penalty_summary = rename_duplicate_columns(penalty_summary)\n",
    "    if goalie_stats is not None:\n",
    "        goalie_stats = rename_duplicate_columns(goalie_stats)\n",
    "    if player_stats is not None:\n",
    "        player_stats = rename_duplicate_columns(player_stats)\n",
    "    if line_chart is not None:\n",
    "        line_chart = rename_duplicate_columns(line_chart)\n",
    "    if linescore is not None:\n",
    "        linescore = rename_duplicate_columns(linescore)\n",
    "\n",
    "    # Parse the advanced metrics page\n",
    "    if metrics_html:\n",
    "        advanced_metrics = parse_new_advanced_metrics(metrics_html)\n",
    "    else:\n",
    "        advanced_metrics = None\n",
    "        \n",
    "    # Return the DataFrames\n",
    "    return game_details, scoring_summary, penalty_summary, goalie_stats, player_stats, line_chart, linescore, advanced_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Conference</th>\n",
       "      <th>Game_Notes</th>\n",
       "      <th>Home_Team</th>\n",
       "      <th>Home_Team_Link</th>\n",
       "      <th>Home_Score</th>\n",
       "      <th>Away_Team</th>\n",
       "      <th>Away_Team_Link</th>\n",
       "      <th>Away_Score</th>\n",
       "      <th>OT</th>\n",
       "      <th>Box_Link</th>\n",
       "      <th>Metrics_Link</th>\n",
       "      <th>Day</th>\n",
       "      <th>Game_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2024-03-16</td>\n",
       "      <td>NCHC Tournament</td>\n",
       "      <td>NCHC Quarterfinal</td>\n",
       "      <td>Omaha</td>\n",
       "      <td>/reports/team/Omaha/37</td>\n",
       "      <td>3</td>\n",
       "      <td>Colorado College</td>\n",
       "      <td>/reports/team/Colorado-College/16</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>/box/final/20240316/uno/cc_/</td>\n",
       "      <td>/box/metrics.php?gd=97733</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2024-03-16-Omaha-Colorado College</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2024-03-16</td>\n",
       "      <td>NCHC Tournament</td>\n",
       "      <td>NCHC Quarterfinal</td>\n",
       "      <td>Minnesota Duluth</td>\n",
       "      <td>/reports/team/Minnesota-Duluth/36</td>\n",
       "      <td>2</td>\n",
       "      <td>Denver</td>\n",
       "      <td>/reports/team/Denver/20</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>/box/final/20240316/mnd/den/</td>\n",
       "      <td>/box/metrics.php?gd=97729</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2024-03-16-Minnesota Duluth-Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2024-03-17</td>\n",
       "      <td>AHA Tournament</td>\n",
       "      <td>Atlantic Hockey Semifinal</td>\n",
       "      <td>American Int'l</td>\n",
       "      <td>/reports/team/American-Intl/5</td>\n",
       "      <td>3</td>\n",
       "      <td>Holy Cross</td>\n",
       "      <td>/reports/team/Holy-Cross/23</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>/box/final/20240317/aic/hcr/</td>\n",
       "      <td>/box/metrics.php?gd=97737</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2024-03-17-American Int'l-Holy Cross</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2024-03-17</td>\n",
       "      <td>NCHC Tournament</td>\n",
       "      <td>NCHC Quarterfinal</td>\n",
       "      <td>Western Michigan</td>\n",
       "      <td>/reports/team/Western-Michigan/57</td>\n",
       "      <td>1</td>\n",
       "      <td>St. Cloud State</td>\n",
       "      <td>/reports/team/St-Cloud-State/52</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>/box/final/20240317/wmu/stc/</td>\n",
       "      <td>/box/metrics.php?gd=97736</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2024-03-17-Western Michigan-St. Cloud State</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2024-03-17</td>\n",
       "      <td>NCHC Tournament</td>\n",
       "      <td>NCHC Quarterfinal</td>\n",
       "      <td>Omaha</td>\n",
       "      <td>/reports/team/Omaha/37</td>\n",
       "      <td>2</td>\n",
       "      <td>Colorado College</td>\n",
       "      <td>/reports/team/Colorado-College/16</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>/box/final/20240317/uno/cc_/</td>\n",
       "      <td>/box/metrics.php?gd=97738</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2024-03-17-Omaha-Colorado College</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date       Conference                 Game_Notes         Home_Team  \\\n",
       "175  2024-03-16  NCHC Tournament          NCHC Quarterfinal             Omaha   \n",
       "176  2024-03-16  NCHC Tournament          NCHC Quarterfinal  Minnesota Duluth   \n",
       "177  2024-03-17   AHA Tournament  Atlantic Hockey Semifinal    American Int'l   \n",
       "178  2024-03-17  NCHC Tournament          NCHC Quarterfinal  Western Michigan   \n",
       "179  2024-03-17  NCHC Tournament          NCHC Quarterfinal             Omaha   \n",
       "\n",
       "                        Home_Team_Link Home_Score         Away_Team  \\\n",
       "175             /reports/team/Omaha/37          3  Colorado College   \n",
       "176  /reports/team/Minnesota-Duluth/36          2            Denver   \n",
       "177      /reports/team/American-Intl/5          3        Holy Cross   \n",
       "178  /reports/team/Western-Michigan/57          1   St. Cloud State   \n",
       "179             /reports/team/Omaha/37          2  Colorado College   \n",
       "\n",
       "                        Away_Team_Link Away_Score OT  \\\n",
       "175  /reports/team/Colorado-College/16          1      \n",
       "176            /reports/team/Denver/20          5      \n",
       "177        /reports/team/Holy-Cross/23          1      \n",
       "178    /reports/team/St-Cloud-State/52          5      \n",
       "179  /reports/team/Colorado-College/16          1      \n",
       "\n",
       "                         Box_Link               Metrics_Link       Day  \\\n",
       "175  /box/final/20240316/uno/cc_/  /box/metrics.php?gd=97733  Saturday   \n",
       "176  /box/final/20240316/mnd/den/  /box/metrics.php?gd=97729  Saturday   \n",
       "177  /box/final/20240317/aic/hcr/  /box/metrics.php?gd=97737    Sunday   \n",
       "178  /box/final/20240317/wmu/stc/  /box/metrics.php?gd=97736    Sunday   \n",
       "179  /box/final/20240317/uno/cc_/  /box/metrics.php?gd=97738    Sunday   \n",
       "\n",
       "                                         Game_ID  \n",
       "175            2024-03-16-Omaha-Colorado College  \n",
       "176           2024-03-16-Minnesota Duluth-Denver  \n",
       "177         2024-03-17-American Int'l-Holy Cross  \n",
       "178  2024-03-17-Western Michigan-St. Cloud State  \n",
       "179            2024-03-17-Omaha-Colorado College  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscraped_games.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Assuming unscraped_games DataFrame is already defined and available\n",
    "# Assuming scrape_game function is defined as shown in your code snippets\n",
    "\n",
    "def store_in_database(df, table_name, conn):\n",
    "    \"\"\"\n",
    "    Store the given DataFrame into the specified table within the database.\n",
    "    Handles the insertion and commits the changes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error inserting data into {table_name}: {e}\")\n",
    "        # Handle or log error as appropriate\n",
    "\n",
    "def scrape_and_store_games(unscraped_games, db_path):\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    for index, game in unscraped_games.iterrows():\n",
    "        game_id = game['Game_ID']\n",
    "        box_link = game['Box_Link']  # Assuming this column name, adjust as necessary\n",
    "        metrics_link = game['Metrics_Link']  # Assuming this column name, adjust as necessary\n",
    "        \n",
    "        # Scrape the game data\n",
    "        game_data = scrape_game(game_id, box_link, metrics_link)\n",
    "        if not game_data:\n",
    "            logging.error(f\"Failed to scrape game {game_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Unpack the scraped data\n",
    "        game_details, scoring_summary, penalty_summary, goalie_stats, player_stats, line_chart, linescore = game_data\n",
    "        \n",
    "        # Store each DataFrame in the respective table\n",
    "        if game_details is not None:\n",
    "            store_in_database(game_details, 'game_details', conn)\n",
    "        # Repeat for other DataFrames (scoring_summary, penalty_summary, etc.)\n",
    "        if scoring_summary is not None:\n",
    "            store_in_database(scoring_summary, 'scoring_summary', conn)\n",
    "        if penalty_summary is not None:\n",
    "            store_in_database(penalty_summary, 'penalty_summary', conn)\n",
    "        if goalie_stats is not None:\n",
    "            store_in_database(goalie_stats, 'goalie_stats', conn)\n",
    "        if player_stats is not None:\n",
    "            store_in_database(player_stats, 'player_stats', conn)\n",
    "        if line_chart is not None:\n",
    "            store_in_database(line_chart, 'line_chart', conn)\n",
    "        if linescore is not None:\n",
    "            store_in_database(linescore, 'linescore', conn)\n",
    "        if advanced_metrics is not None:\n",
    "            store_in_database(advanced_metrics, 'advanced_metrics', conn)\n",
    "\n",
    "        \n",
    "    # Close the database connection\n",
    "    conn.close()\n",
    "\n",
    "# Call the function with the unscraped_games DataFrame and database path\n",
    "scrape_and_store_games(unscraped_games, db_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scrape the games from the filtered list above and store the data in the database\n",
    "# # Initialize an empty list to store the DataFrames\n",
    "# game_details_list = []\n",
    "# scoring_summary_list = []\n",
    "# penalty_summary_list = []\n",
    "# goalie_stats_list = []\n",
    "# player_stats_list = []\n",
    "# line_chart_list = []\n",
    "# linescore_list = []\n",
    "\n",
    "# # Loop through the unscraped games\n",
    "# for row in unscraped_games.itertuples():\n",
    "#     game_id = row.Game_ID\n",
    "#     box_link = row.Box_Link\n",
    "#     metrics_link = row.Metrics_Link\n",
    "\n",
    "#     # Scrape the game if the box score links are valid, otherwise skip\n",
    "#     if box_link:\n",
    "#         # Check if the metrics link is valid\n",
    "#         if metrics_link:\n",
    "#             game_details, scoring_summary, penalty_summary, goalie_stats, player_stats, line_chart, linescore = scrape_game(game_id, box_link, metrics_link)\n",
    "    \n",
    "    \n",
    "\n",
    "#     # game_details, scoring_summary, penalty_summary, goalie_stats, player_stats, line_chart, linescore = scrape_game(game_id, box_link, metrics_link)\n",
    "\n",
    "#     # Append the DataFrames to the lists\n",
    "#     game_details_list.append(game_details)\n",
    "#     scoring_summary_list.append(scoring_summary)\n",
    "#     penalty_summary_list.append(penalty_summary)\n",
    "#     goalie_stats_list.append(goalie_stats)\n",
    "#     player_stats_list.append(player_stats)\n",
    "#     line_chart_list.append(line_chart)\n",
    "#     linescore_list.append(linescore)\n",
    "\n",
    "# # Connect to the database\n",
    "# conn = sqlite3.connect(db_path)\n",
    "\n",
    "# # Loop through the DataFrames and insert the data into the database\n",
    "# for game_details, scoring_summary, penalty_summary, goalie_stats, player_stats, line_chart, linescore in zip(game_details_list, scoring_summary_list, penalty_summary_list, goalie_stats_list, player_stats_list, line_chart_list, linescore_list):\n",
    "#     # Insert the game details\n",
    "#     if game_details is not None:\n",
    "#         game_details.to_sql('game_details', conn, if_exists='append', index=False)\n",
    "    \n",
    "#     # Insert the scoring summary\n",
    "#     if scoring_summary is not None:\n",
    "#         scoring_summary.to_sql('scoring_summary', conn, if_exists='append', index=False)\n",
    "    \n",
    "#     # Insert the penalty summary\n",
    "#     if penalty_summary is not None:\n",
    "#         penalty_summary.to_sql('penalty_summary', conn, if_exists='append', index=False)\n",
    "    \n",
    "#     # Insert the goalie stats\n",
    "#     if goalie_stats is not None:\n",
    "#         goalie_stats.to_sql('goalie_stats', conn, if_exists='append', index=False)\n",
    "    \n",
    "#     # Insert the player stats\n",
    "#     if player_stats is not None:\n",
    "#         player_stats.to_sql('player_stats', conn, if_exists='append', index=False)\n",
    "    \n",
    "#     # Insert the line chart\n",
    "#     if line_chart is not None:\n",
    "#         line_chart.to_sql('line_chart', conn, if_exists='append', index=False)\n",
    "    \n",
    "#     # Insert the linescore\n",
    "#     if linescore is not None:\n",
    "#         linescore.to_sql('linescore', conn, if_exists='append', index=False)\n",
    "# # Close the connection\n",
    "# conn.close()\n",
    "\n",
    "# # Print the number of games scraped\n",
    "# print(f'Number of games scraped: {len(unscraped_games)}')\n",
    "\n",
    "# # Log the number of games scraped\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
