{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Book to collect data about the current college hockey season from College Hockey News\n",
    "\n",
    "## Dependencies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "FILE_TAG = 'FEB_25_Current_YTD_Stats'\n",
    "\n",
    "## global variables\n",
    "# # current_year_url = 'https://www.collegehockeynews.com/schedules/?season=20232024'\n",
    "# tag = '2023_Season_Nov 2'\n",
    "\n",
    "##### TEMP####\n",
    "## global variables\n",
    "current_year_url = 'https://www.collegehockeynews.com/schedules/?season=20232024'\n",
    "# tag = 'NOV_13_Current Season YTD'\n",
    "tag = FILE_TAG\n",
    "\n",
    "result_table_name = '2023'\n",
    "\n",
    "\n",
    "## Base usl for box scores and metrics\n",
    "\n",
    "base_url = 'https://www.collegehockeynews.com'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a table of every game in CHN database for the selected season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions\n",
    "### Parse the current season schedule / results page\n",
    "\n",
    "def parse_current_season(url):\n",
    "        # Initialize variables\n",
    "    current_date = None\n",
    "    current_conference = None\n",
    "    game_notes = None\n",
    "\n",
    "    # Initialize an empty list to hold the data\n",
    "    data = []\n",
    "\n",
    "    # Parse the page with BeautifulSoup\n",
    "    # Get the page with requests\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # select the table or tables\n",
    "    tables = soup.find_all('table')\n",
    "\n",
    "    rows = soup.find_all('tr')\n",
    "\n",
    "    # Loop through each row to find relevant information\n",
    "    for row in rows:\n",
    "        # Check for date row\n",
    "        if row.get('class') == ['stats-section']:\n",
    "            current_date = row.find('td').text.strip()\n",
    "        # Check for conference row\n",
    "        elif row.get('class') == ['sked-header']:\n",
    "            current_conference = row.find('td').text.strip()\n",
    "        # Check for game notes\n",
    "        elif len(row.find_all('td')) == 2:\n",
    "            game_notes = row.find_all('td')[1].text.strip()\n",
    "        # Process rows with game data\n",
    "        elif row.get('valign') == 'top':\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 9:\n",
    "                home_team = cells[0].text.strip()\n",
    "                # Remove any hyphens from the team name\n",
    "                home_team = home_team.replace('-', ' ')\n",
    "                home_team_link = cells[0].find('a')['href'] if cells[0].find('a') else None\n",
    "                home_score = cells[1].text.strip()\n",
    "                away_team = cells[3].text.strip()\n",
    "                away_team_link = cells[3].find('a')['href'] if cells[3].find('a') else None\n",
    "                away_score = cells[4].text.strip()\n",
    "                ot = cells[5].text.strip()\n",
    "                box_link = cells[7].find('a')['href'] if cells[7].find('a') else None\n",
    "                metrics_link = cells[8].find('a')['href'] if cells[8].find('a') else None\n",
    "                # Capture Game Notes\n",
    "                game_notes_cell = cells[-1].find('small')\n",
    "                game_notes = game_notes_cell.text.strip() if game_notes_cell else None\n",
    "\n",
    "                # Append data to the list\n",
    "                data.append([current_date, current_conference, game_notes, home_team, home_team_link, home_score, away_team, away_team_link, away_score, ot, box_link, metrics_link])\n",
    "                game_notes = None  # Reset game notes for the next row\n",
    "    return data\n",
    "\n",
    "## call the function\n",
    "data = parse_current_season(current_year_url)\n",
    "\n",
    "\n",
    "# Create a dataframe from the list\n",
    "\n",
    "columns = ['Date', 'Conference', 'Game_Notes', 'Home_Team', 'Home_Team_Link', 'Home_Score', 'Away_Team', 'Away_Team_Link', 'Away_Score', 'OT', 'Box_Link', 'Metrics_Link']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "            \n",
    "## Extract the day of the week from the date and save in new column\n",
    "df['Day'] = pd.to_datetime(df['Date']).dt.day_name()\n",
    "# remove day of the week from date\n",
    "# format data column as YYYY-MM-DD\n",
    "df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "### Create a new column for the game ID\n",
    "## Game ID will be a combination of the date and abbreviated team names\n",
    "\n",
    "# Function to abbreviate the team names\n",
    "for row in df.itertuples():\n",
    "    home_team = row.Home_Team\n",
    "    away_team = row.Away_Team\n",
    "    home_team_abbr = home_team.split(' ')[-1]\n",
    "    away_team_abbr = away_team.split(' ')[-1]\n",
    "    # Remove any hyphens from the team name if there are any\n",
    "    home_team_abbr = home_team_abbr.replace('-', ' ')\n",
    "    away_team_abbr = away_team_abbr.replace('-', ' ')\n",
    "    game_id = f'{row.Date}-{home_team_abbr}-{away_team_abbr}'\n",
    "    df.loc[row.Index, 'Game_ID'] = game_id\n",
    "\n",
    "# Create a new column for the game ID\n",
    "df['Game_ID'] = df['Game_ID'].str.replace(',', '')\n",
    "\n",
    "# Remove any hyphens from the team names if any\n",
    "df['Home_Team'] = df['Home_Team'].str.replace('-', ' ')\n",
    "df['Away_Team'] = df['Away_Team'].str.replace('-', ' ')\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['Game_ID'] = df.apply(lambda row: f'{row.Date}-{row.Home_Team}-{row.Away_Team}', axis=1)\n",
    "\n",
    "## Filter out games that have not been played yet\n",
    "df = df[df['Home_Score'] != '']\n",
    "\n",
    "# Replace Nan values in metrics column with empty string\n",
    "df['Metrics_Link'] = df['Metrics_Link'].fillna('')\n",
    "\n",
    "# save the dataframe to a csv file for manual review\n",
    "df.to_csv('../TEMP/season_table_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df.tail(10)\n",
    "\n",
    "## Save csv of just the current season results\n",
    "df.to_csv(f'../TEMP/{result_table_name}.csv', index=False)\n",
    "\n",
    "# Store the dataframe as games_df\n",
    "games_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for parsing the box score and metrics pages\n",
    "\n",
    "# Initialize logging for Error and Warning messages\n",
    "logging.basicConfig(filename='../TEMP/current_scrape.log', level=logging.INFO)\n",
    "\n",
    "#### PARSE PLAYER STATS TABLE ####\n",
    "def parse_player_summary(html_content):\n",
    "    # Initialize BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find the playersums div\n",
    "    playersums_div = soup.find('div', id='playersums')\n",
    "    if playersums_div is None:\n",
    "        return \"Player summaries div not found\"\n",
    "\n",
    "    # Initialize list to store player stats\n",
    "    player_stats = []\n",
    "\n",
    "    # Loop through each playersum div\n",
    "    for player_sum in playersums_div.find_all('div', class_='playersum'):\n",
    "        team = player_sum.find('td').text.strip()\n",
    "        \n",
    "        # Loop through table rows\n",
    "        for row in player_sum.find_all('tr'):\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 1:\n",
    "                player = cols[0].text.strip()\n",
    "                goals = cols[1].text.strip()\n",
    "                assists = cols[2].text.strip()\n",
    "                points = cols[3].text.strip()\n",
    "                plus_minus = cols[4].text.strip()\n",
    "                shots = cols[5].text.strip()\n",
    "                pim = cols[6].text.strip()\n",
    "                fowl = cols[7].text.strip() if len(cols) > 7 else None\n",
    "                \n",
    "                fow, fol = None, None\n",
    "                win_percentage = None\n",
    "                \n",
    "                \n",
    "\n",
    "                try:\n",
    "                    if fowl and '‑' in fowl:  # Checking if it contains a hyphen\n",
    "                        fow, fol = map(int, fowl.split('‑'))\n",
    "                        total_fo = fow + fol\n",
    "                        win_percentage = (fow / total_fo) * 100 if total_fo > 0 else 0\n",
    "                except ValueError:\n",
    "                    fow, fol, win_percentage = None, None, None\n",
    "\n",
    "                \n",
    "\n",
    "                \n",
    "                player_stat = {\n",
    "                    'Team': team,\n",
    "                    'Player': player,\n",
    "                    'G': goals,\n",
    "                    'A': assists,\n",
    "                    'Pt.': points,\n",
    "                    '+/-': plus_minus,\n",
    "                    'Sh': shots,\n",
    "                    'PIM': pim,\n",
    "                    'FOW': fow,\n",
    "                    'FOL': fol,\n",
    "                    'FO%': win_percentage\n",
    "                }\n",
    "                player_stats.append(player_stat)\n",
    "\n",
    "    return pd.DataFrame(player_stats)\n",
    "\n",
    "\n",
    "############# PARSEING SCORING SUMMARY WITH BS4\n",
    "\n",
    "def parse_scoring_summary(html_content):\n",
    "    # Initialize BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the scoring div and table\n",
    "    scoring_div = soup.find('div', id='scoring')\n",
    "    if scoring_div is None:\n",
    "        logging.error(\"Scoring div not found\")\n",
    "        return None\n",
    "\n",
    "    scoring_table = scoring_div.find('table')\n",
    "    if scoring_table is None:\n",
    "        logging.error(\"Scoring table not found within the scoring div\")\n",
    "        return None\n",
    "\n",
    "    # Initialize list to store scoring events\n",
    "    scoring_events = []\n",
    "    period = None\n",
    "\n",
    "    # Loop through table rows\n",
    "    for row in scoring_table.find_all('tr'):\n",
    "        if 'stats-section' in row.get('class', []):\n",
    "            td = row.find('td')\n",
    "            if td:\n",
    "                period = td.text.strip()\n",
    "            else:\n",
    "                logging.warning(\"Period name not found in 'stats-section' row\")\n",
    "                period = \"Unknown\"\n",
    "        else:\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 1:\n",
    "                try:\n",
    "                    team = cols[0].text.strip()\n",
    "                    pp = cols[1].text.strip()\n",
    "\n",
    "                    player_data = cols[3].text.strip()\n",
    "                    match = re.match(r\"(.+)\\s\\((\\d+)\\)\", player_data)\n",
    "                    player = match.group(1) if match else player_data\n",
    "                    goals = int(match.group(2)) if match else None\n",
    "\n",
    "                    assist_data_raw = cols[4].text.strip()\n",
    "                    assist_data = assist_data_raw.split(\", \") if assist_data_raw else []\n",
    "                    assist1 = assist_data[0] if len(assist_data) > 0 else None\n",
    "                    assist2 = assist_data[1] if len(assist_data) > 1 else None\n",
    "\n",
    "                    time = cols[5].text.strip()\n",
    "\n",
    "                    scoring_event = {\n",
    "                        'Period': period,\n",
    "                        'Team': team,\n",
    "                        'PP': pp,\n",
    "                        'Player': player,\n",
    "                        'Player_Goals': goals,\n",
    "                        'Assist1': assist1,\n",
    "                        'Assist2': assist2,\n",
    "                        'Time': time\n",
    "                    }\n",
    "                    scoring_events.append(scoring_event)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"An error occurred while parsing a scoring event row: {e}\")\n",
    "            else:\n",
    "                logging.warning(f\"Insufficient columns in scoring row: {len(cols)}\")\n",
    "\n",
    "    return pd.DataFrame(scoring_events)\n",
    "\n",
    "\n",
    "############# PARSEING PENALTY SUMMARY WITH BS4\n",
    "\n",
    "def parse_penalty_summary(html_content):\n",
    "    # Initialize BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the penalties div and table\n",
    "    penalties_div = soup.find('div', id='penalties')\n",
    "    if penalties_div is None:\n",
    "        logging.error(\"Penalties div not found\")\n",
    "        return None\n",
    "\n",
    "    penalties_table = penalties_div.find('table')\n",
    "    if penalties_table is None:\n",
    "        logging.error(\"Penalties table not found within the penalties div\")\n",
    "        return None\n",
    "\n",
    "    # Initialize list to store penalty events\n",
    "    penalty_events = []\n",
    "    period = None\n",
    "\n",
    "    # Loop through table rows\n",
    "    for row in penalties_table.find_all('tr'):\n",
    "        if 'stats-section' in row.get('class', []):\n",
    "            td = row.find('td')\n",
    "            if td:\n",
    "                period = td.text.strip()\n",
    "            else:\n",
    "                logging.warning(\"Period name not found in 'stats-section' row\")\n",
    "                period = \"Unknown\"\n",
    "        else:\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 1:\n",
    "                team = cols[0].text.strip()\n",
    "                player = cols[1].text.strip()\n",
    "                pen_length = cols[2].text.strip()\n",
    "                penalty_type = cols[3].text.strip()\n",
    "                time = cols[4].text.strip()\n",
    "\n",
    "                penalty_event = {\n",
    "                    'Period': period,\n",
    "                    'Team': team,\n",
    "                    'Player': player,\n",
    "                    'Pen_Length': pen_length,\n",
    "                    'Penalty_Type': penalty_type,\n",
    "                    'Time': time\n",
    "                }\n",
    "                penalty_events.append(penalty_event)\n",
    "\n",
    "    return pd.DataFrame(penalty_events)\n",
    "\n",
    "\n",
    "############# PARSEING GOALIE SUMMARY WITH BS4\n",
    "\n",
    "def parse_goalie_stats(html_content):\n",
    "    # Initialize BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the goalies div and table\n",
    "    goalies_div = soup.find('div', id='goalies')\n",
    "    if goalies_div is None:\n",
    "        logging.error(\"Goalies div not found\")\n",
    "        return None\n",
    "\n",
    "    goalies_table = goalies_div.find('table')\n",
    "    if goalies_table is None:\n",
    "        logging.error(\"Goalies table not found within the goalies div\")\n",
    "        return None\n",
    "\n",
    "    # Initialize list to store goalie stats\n",
    "    goalie_stats = []\n",
    "    team = None\n",
    "\n",
    "    # Loop through table rows\n",
    "    for row in goalies_table.find_all('tr'):\n",
    "        if 'stats-header' in row.get('class', []):\n",
    "            td = row.find('td')\n",
    "            if td:\n",
    "                team = td.text.strip()\n",
    "            else:\n",
    "                logging.warning(\"Team name not found in 'stats-header' row\")\n",
    "                team = \"Unknown\"\n",
    "        else:\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 1:\n",
    "                goalie = cols[0].text.strip()\n",
    "                sv = cols[1].text.strip()\n",
    "                ga = cols[2].text.strip()\n",
    "                minutes = cols[3].text.strip()\n",
    "\n",
    "                goalie_stat = {\n",
    "                    'Team': team,\n",
    "                    'Goalie': goalie,\n",
    "                    'SV': sv,\n",
    "                    'GA': ga,\n",
    "                    'Minutes': minutes\n",
    "                }\n",
    "                goalie_stats.append(goalie_stat)\n",
    "\n",
    "    return pd.DataFrame(goalie_stats)\n",
    "\n",
    "\n",
    "#### PARSE THE ADVANCED TEAM METRICS TABLES ####\n",
    "### NEW - IT WORKS IN THE NOTEBOOK BUT NOT IN THE FUNCTION\n",
    "### RETURNS WHOLE ADVANCED METRICS AS SINGLE TABLE\n",
    "####################################\n",
    "def parse_new_advanced_metrics(html_content):\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all tables with advanced metrics\n",
    "    tables = soup.find_all('table', {'class': 'sortable metrics'})\n",
    "    \n",
    "    # List to store all parsed data\n",
    "    all_data = []\n",
    "    \n",
    "    for table in tables:\n",
    "        # Extract team name from the table header\n",
    "        team_name = table.find('td').text.strip()\n",
    "        \n",
    "        # Extract headers (skipping the Player header)\n",
    "        headers = [header.text for header in table.find_all('th')][1:]\n",
    "        \n",
    "        # Prepare final column headers\n",
    "        col_names = ['Team', 'Player']\n",
    "        for header in headers:\n",
    "            col_names.append(header)\n",
    "        \n",
    "        # Extract player data\n",
    "        rows = table.find_all('tr')[2:]  # skipping the two header rows\n",
    "        for row in rows:\n",
    "            player_data = [team_name]  # start with team name\n",
    "            cells = row.find_all('td')\n",
    "            player_data.append(cells[0].text.strip())  # player name\n",
    "            for cell in cells[1:]:\n",
    "                player_data.append(cell.text.strip())\n",
    "            all_data.append(player_data)\n",
    "    \n",
    "    # Convert the list of data to a DataFrame\n",
    "    df = pd.DataFrame(all_data, columns=col_names)\n",
    "    return df\n",
    "\n",
    "# Testing the new function with the provided HTML content\n",
    "# advanced_df = parse_new_advanced_metrics(advanced_tie_game_html_content)\n",
    "# advanced_df\n",
    "\n",
    "\n",
    "######## NEWS TEST ###############  \n",
    "def parse_advanced_metrics_tables(html_content):\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all tables with advanced metrics\n",
    "    tables = soup.find_all('table', {'class': 'sortable metrics'})\n",
    "    \n",
    "    # List to store all parsed data\n",
    "    all_data = []\n",
    "    \n",
    "    for table in tables:\n",
    "        # Extract team name from the table header\n",
    "        team_name = table.find('td').text.strip()\n",
    "        \n",
    "        # Extract headers (skipping the Player header)\n",
    "        headers = [header.text for header in table.find_all('th')][1:]\n",
    "        \n",
    "        # Prepare final column headers\n",
    "        col_names = ['Team', 'Player']\n",
    "        for header in headers:\n",
    "            col_names.append(header)\n",
    "        \n",
    "        # Extract player data\n",
    "        rows = table.find_all('tr')[2:]  # skipping the two header rows\n",
    "        for row in rows:\n",
    "            player_data = [team_name]  # start with team name\n",
    "            cells = row.find_all('td')\n",
    "            player_data.append(cells[0].text.strip())  # player name\n",
    "            for cell in cells[1:]:\n",
    "                player_data.append(cell.text.strip())\n",
    "            all_data.append(player_data)\n",
    "    \n",
    "    # Convert the list of data to a DataFrame\n",
    "    df = pd.DataFrame(all_data, columns=col_names)\n",
    "    return df\n",
    "\n",
    "\n",
    "######### OLDER FUNCTION for Test ################\n",
    "# ########### UPDATED FUNCTION TO STORE TEAM NAMES IN THE DATAFRAMES\n",
    "# def parse_advanced_metrics_tables(html_content):\n",
    "#     # Initialize list to store DataFrames\n",
    "#     dfs = []\n",
    "    \n",
    "#     # Parse HTML content\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "#     # Find all tables\n",
    "#     tables = soup.find_all('table', {'class': 'sortable metrics'})\n",
    "    \n",
    "#     for table in tables:\n",
    "#         # Extract team name\n",
    "#         team_name = table.find('td').text\n",
    "        \n",
    "#         # Initialize list to store column names and data\n",
    "#         col_names = []\n",
    "#         col_names_final = []\n",
    "#         data = []\n",
    "        \n",
    "#         # Get headers\n",
    "#         headers = table.find_all('th')\n",
    "#         for header in headers:\n",
    "#             col_names.append(header.text)\n",
    "        \n",
    "#         # Add TOTAL, EVEN STRENGTH, POWER PLAY, CLOSE to column names\n",
    "#         section_headers = ['TOTAL', 'EVEN STRENGTH', 'POWER PLAY', 'CLOSE']\n",
    "#         for col in col_names:\n",
    "#             for section in section_headers:\n",
    "#                 if col in section_headers:\n",
    "#                     temp_col = section\n",
    "#                 else:\n",
    "#                     temp_col = f\"{section}_{col}\"\n",
    "#             col_names_final.append(temp_col)\n",
    "        \n",
    "#         # Get data rows\n",
    "#         rows = table.find_all('tr')[2:]  # skip header rows\n",
    "#         for row in rows:\n",
    "#             row_data = [team_name]  # Add team name as the first element\n",
    "#             cells = row.find_all('td')\n",
    "#             for cell in cells:\n",
    "#                 row_data.append(cell.text.strip())\n",
    "#             data.append(row_data)\n",
    "        \n",
    "#         # Add \"Team\" to the column names\n",
    "#         new_names = ['Team', 'Player', 'TOTAL_Block', 'TOTAL_Miss', 'TOTAL_Saved', 'TOTAL_Goals', 'TOTAL_Total_Shots',\n",
    "#                      'EVEN_Block', 'EVEN_Miss', 'EVEN_Saved', 'EVEN_Goals', 'EVEN_Total_Shots',\n",
    "#                      'PP_Block', 'PP_Miss', 'PP_Saved', 'PP_Goals', 'PP_Total_Shots',\n",
    "#                      'CLOSE_Block', 'CLOSE_Miss', 'CLOSE_Saved', 'CLOSE_Goals', 'CLOSE_Total_Shots',\n",
    "#                      'D_Blocks', 'Faceoffs']\n",
    "\n",
    "#         # Create DataFrame and set the column names\n",
    "#         df = pd.DataFrame(data, columns=new_names)\n",
    "\n",
    "#         # Append DataFrame to list\n",
    "#         dfs.append(df)\n",
    "    \n",
    "#     return dfs\n",
    "\n",
    "\n",
    "# Complete code for parsing the line chart information with specific positions for forwards and defensemen.\n",
    "\n",
    "\n",
    "def parse_line_chart(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    line_chart_div = soup.find('div', id='linechart')\n",
    "\n",
    "    if line_chart_div is None:\n",
    "        logging.error(\"Line chart div not found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    line_data = []\n",
    "\n",
    "    for team_div in line_chart_div.find_all('div', recursive=False):\n",
    "        h3 = team_div.find('h3')\n",
    "        if h3 is None:\n",
    "            logging.warning(\"Team name not found\")\n",
    "            continue\n",
    "        \n",
    "        team_name = h3.text.strip()\n",
    "        \n",
    "        for line_type_div in team_div.find_all('div', recursive=False):\n",
    "            line_type = line_type_div.get('class')[0] if line_type_div.get('class') else None\n",
    "            if line_type is None:\n",
    "                logging.warning(\"Line type not found\")\n",
    "                continue\n",
    "            \n",
    "            if line_type == 'f':\n",
    "                position_types = ['Left Wing', 'Center', 'Right Wing']\n",
    "            elif line_type == 'd':\n",
    "                position_types = ['Left D', 'Right D']\n",
    "            elif line_type == 'x':\n",
    "                position_types = ['Extra']\n",
    "            elif line_type == 'g':\n",
    "                position_types = ['Goalie']\n",
    "                goalie_count = 1  # Initialize goalie count\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            players = line_type_div.find_all('div')\n",
    "            if not players:\n",
    "                logging.warning(f\"No players found for {team_name} in {line_type}\")\n",
    "                continue\n",
    "            \n",
    "            for i, player in enumerate(players):\n",
    "                player_name = player.text.strip()\n",
    "                if line_type == 'x':\n",
    "                    player_name = player_name.split(' ')[0]\n",
    "                if line_type == 'g':\n",
    "                    line_number = f\"Goalie {goalie_count}\"\n",
    "                    goalie_count += 1\n",
    "                else:\n",
    "                    line_number = i // len(position_types) + 1\n",
    "\n",
    "                position = position_types[i % len(position_types)]\n",
    "                line_data.append({\n",
    "                    'Team': team_name,\n",
    "                    'Line': line_number,\n",
    "                    'Position': position,\n",
    "                    'Player': player_name\n",
    "                })\n",
    "\n",
    "    if not line_data:\n",
    "        logging.error(\"No line data was collected\")\n",
    "\n",
    "    df = pd.DataFrame(line_data)\n",
    "    \n",
    "    # # Log DataFrame info for debugging\n",
    "    # if df.empty:\n",
    "    #     logging.warning(\"Generated line chart DataFrame is empty.\")\n",
    "    # else:\n",
    "    #     logging.info(f\"Generated line chart DataFrame with columns: {df.columns.tolist()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "### Get the Linescore Elements - Score, shots, ect by period####\n",
    "### UPDATED \n",
    "\n",
    "def parse_linescore(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    linescore_data = []\n",
    "    \n",
    "    # Parsing the Goals table\n",
    "    goals_table = soup.select_one(\"#goals table\")\n",
    "    if goals_table is None:\n",
    "        logging.error(\"Goals table not found\")\n",
    "        return None\n",
    "    \n",
    "    rows = goals_table.select('tbody tr')\n",
    "    if not rows:\n",
    "        logging.warning(\"No rows found in Goals table\")\n",
    "        return None\n",
    "    \n",
    "    for row in rows:\n",
    "        team_data = {}\n",
    "        td = row.select_one('td')\n",
    "        if td:\n",
    "            team_data['Team'] = td.text\n",
    "        else:\n",
    "            logging.warning(\"Team name not found in Goals table\")\n",
    "            continue\n",
    "\n",
    "        goals = row.select('td')[1:]\n",
    "        for i, goal in enumerate(goals):\n",
    "            period = i + 1\n",
    "            column_name = f'goals{period}' if i < len(goals) - 1 else 'goalsT'\n",
    "            team_data[column_name] = int(goal.text)\n",
    "        \n",
    "        linescore_data.append(team_data)\n",
    "    \n",
    "\n",
    "    # Parsing the Shots table\n",
    "    shots_table = soup.select_one(\"#shots table\")\n",
    "    if shots_table is None:\n",
    "        logging.error(\"Shots table not found\")\n",
    "        return None\n",
    "\n",
    "    rows = shots_table.select('tbody tr')\n",
    "    if not rows:\n",
    "        logging.warning(\"No rows found in Shots table\")\n",
    "        return None\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        shots = row.select('td')[1:]\n",
    "        if not shots:\n",
    "            logging.warning(f\"No shot data found for row {i+1} in Shots table\")\n",
    "            continue\n",
    "\n",
    "        for j, shot in enumerate(shots):\n",
    "            period = j + 1\n",
    "            column_name = f'shots{period}' if j < len(shots) - 1 else 'shotsT'\n",
    "            try:\n",
    "                linescore_data[i][column_name] = int(shot.text.strip())\n",
    "            except ValueError:\n",
    "                logging.warning(f\"Could not convert shot data to integer for row {i+1}, column {j+1}\")\n",
    "                linescore_data[i][column_name] = None\n",
    "\n",
    "    # Parsing the PP table\n",
    "    pp_table = soup.select_one(\"#pp table\")\n",
    "    if pp_table is None:\n",
    "        logging.error(\"PP table not found\")\n",
    "        return None\n",
    "\n",
    "    rows = pp_table.select('tbody tr')\n",
    "    if not rows:\n",
    "        logging.warning(\"No rows found in PP table\")\n",
    "        return None\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        try:\n",
    "            pen_pim = row.select('td')[1].text.split('‑')\n",
    "            linescore_data[i]['Pen'] = int(pen_pim[0])\n",
    "            linescore_data[i]['PIM'] = int(pen_pim[1])\n",
    "\n",
    "            ppg_ppo = row.select('td')[2].text.split('‑')\n",
    "            linescore_data[i]['PPG'] = int(ppg_ppo[0])\n",
    "            linescore_data[i]['PPO'] = int(ppg_ppo[1])\n",
    "\n",
    "            fow_fol = row.select('td')[3].text.split('‑')\n",
    "            linescore_data[i]['FOW'] = int(fow_fol[0])\n",
    "            linescore_data[i]['FOL'] = int(fow_fol[1])\n",
    "            linescore_data[i]['FOW%'] = (linescore_data[i]['FOW'] / (linescore_data[i]['FOW'] + linescore_data[i]['FOL'])) * 100\n",
    "\n",
    "        except (ValueError, IndexError) as e:\n",
    "            logging.warning(f\"Could not process PP data for row {i+1}. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Convert to DataFrame early\n",
    "    df = pd.DataFrame(linescore_data)\n",
    "\n",
    "    # Ensure all columns exist\n",
    "    expected_goals_columns = [f'goals{i}' for i in range(1, 5)] + ['goalsT']\n",
    "    expected_shots_columns = [f'shots{i}' for i in range(1, 5)] + ['shotsT']\n",
    "\n",
    "    for col in expected_goals_columns + expected_shots_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    # Return the final DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "# ############################## ORIGINAL FUNCTION #########################################\n",
    "# def parse_linescore(html_content):\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "#     linescore_data = []\n",
    "    \n",
    "#     # Parsing the Goals table\n",
    "#     goals_table = soup.select_one(\"#goals table\")\n",
    "#     if goals_table is None:\n",
    "#         logging.error(\"Goals table not found\")\n",
    "#         return None\n",
    "    \n",
    "#     rows = goals_table.select('tbody tr')\n",
    "#     if not rows:\n",
    "#         logging.warning(\"No rows found in Goals table\")\n",
    "#         return None\n",
    "    \n",
    "#     for row in rows:\n",
    "#         team_data = {}\n",
    "#         td = row.select_one('td')\n",
    "#         if td:\n",
    "#             team_data['Team'] = td.text\n",
    "#         else:\n",
    "#             logging.warning(\"Team name not found in Goals table\")\n",
    "#             continue\n",
    "\n",
    "#         goals = row.select('td')[1:]\n",
    "#         for i, goal in enumerate(goals):\n",
    "#             period = i + 1\n",
    "#             column_name = f'goals{period}' if i < len(goals) - 1 else 'goalsT'\n",
    "#             team_data[column_name] = int(goal.text)\n",
    "        \n",
    "#         linescore_data.append(team_data)\n",
    "    \n",
    "\n",
    "#     # Parsing the Shots table\n",
    "#     shots_table = soup.select_one(\"#shots table\")\n",
    "#     if shots_table is None:\n",
    "#         logging.error(\"Shots table not found\")\n",
    "#         return None\n",
    "\n",
    "#     rows = shots_table.select('tbody tr')\n",
    "#     if not rows:\n",
    "#         logging.warning(\"No rows found in Shots table\")\n",
    "#         return None\n",
    "\n",
    "#     for i, row in enumerate(rows):\n",
    "#         shots = row.select('td')[1:]\n",
    "#         if not shots:\n",
    "#             logging.warning(f\"No shot data found for row {i+1} in Shots table\")\n",
    "#             continue\n",
    "\n",
    "#         for j, shot in enumerate(shots):\n",
    "#             period = j + 1\n",
    "#             column_name = f'shots{period}' if j < len(shots) - 1 else 'shotsT'\n",
    "#             try:\n",
    "#                 linescore_data[i][column_name] = int(shot.text.strip())\n",
    "#             except ValueError:\n",
    "#                 logging.warning(f\"Could not convert shot data to integer for row {i+1}, column {j+1}\")\n",
    "#                 linescore_data[i][column_name] = None\n",
    "\n",
    "#     # Parsing the PP table\n",
    "#     pp_table = soup.select_one(\"#pp table\")\n",
    "#     if pp_table is None:\n",
    "#         logging.error(\"PP table not found\")\n",
    "#         return None\n",
    "\n",
    "#     rows = pp_table.select('tbody tr')\n",
    "#     if not rows:\n",
    "#         logging.warning(\"No rows found in PP table\")\n",
    "#         return None\n",
    "\n",
    "#     for i, row in enumerate(rows):\n",
    "#         try:\n",
    "#             pen_pim = row.select('td')[1].text.split('‑')\n",
    "#             linescore_data[i]['Pen'] = int(pen_pim[0])\n",
    "#             linescore_data[i]['PIM'] = int(pen_pim[1])\n",
    "\n",
    "#             ppg_ppo = row.select('td')[2].text.split('‑')\n",
    "#             linescore_data[i]['PPG'] = int(ppg_ppo[0])\n",
    "#             linescore_data[i]['PPO'] = int(ppg_ppo[1])\n",
    "\n",
    "#             fow_fol = row.select('td')[3].text.split('‑')\n",
    "#             linescore_data[i]['FOW'] = int(fow_fol[0])\n",
    "#             linescore_data[i]['FOL'] = int(fow_fol[1])\n",
    "#             linescore_data[i]['FOW%'] = (linescore_data[i]['FOW'] / (linescore_data[i]['FOW'] + linescore_data[i]['FOL'])) * 100\n",
    "\n",
    "#         except (ValueError, IndexError) as e:\n",
    "#             logging.warning(f\"Could not process PP data for row {i+1}. Error: {e}\")\n",
    "#             continue\n",
    "\n",
    "#     return pd.DataFrame(linescore_data)\n",
    "\n",
    "\n",
    "\n",
    "# Function to parse game details\n",
    "\n",
    "\n",
    "def parse_game_details(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    meta_div = soup.find('div', {'id': 'meta'})\n",
    "    if meta_div is None:\n",
    "        logging.error(\"Meta div not found\")\n",
    "        return None\n",
    "    \n",
    "    game_details_div = meta_div.find_all('div')[-1]\n",
    "    if game_details_div is None:\n",
    "        logging.error(\"Game details div not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        date_str = game_details_div.h4.string\n",
    "        day_of_week, date = date_str.split(\", \", 1)\n",
    "        \n",
    "        p_elements = game_details_div.find_all('p')\n",
    "        \n",
    "        # Extract conference and location details\n",
    "        for p in p_elements:\n",
    "            if \"Game\" in p.text:  # e.g., \"Big Ten Game\"\n",
    "                details_strs = p.get_text(separator='|').split('|')\n",
    "                conference = details_strs[0]\n",
    "                location = details_strs[-1].split('at ')[-1]\n",
    "                break\n",
    "        else:  # Defaults if not found\n",
    "            conference, location = None, None\n",
    "        \n",
    "        # Extract referees and assistant referees details\n",
    "        for p in p_elements:\n",
    "            if \"Referees\" in p.text:\n",
    "                refs_str = p.strong.next_sibling if p.strong else None\n",
    "                asst_refs_str = p.find_all('strong')[1].next_sibling if len(p.find_all('strong')) > 1 else None\n",
    "                break\n",
    "        else:  # Defaults if not found\n",
    "            refs_str, asst_refs_str = None, None\n",
    "        \n",
    "        refs = refs_str.split(', ') if refs_str else []\n",
    "        asst_refs = asst_refs_str.split(', ') if asst_refs_str else []\n",
    "        refs = [re.sub(r'[^a-zA-Z ]+', '', ref).strip() for ref in refs]\n",
    "        asst_refs = [re.sub(r'[^a-zA-Z ]+', '', ref).strip() for ref in asst_refs]\n",
    "        \n",
    "        # Extract attendance details using regex for better accuracy\n",
    "        attendance_pattern = r\"Attendance:\\s?(\\d+[\\d,]*)\"\n",
    "        attendance_match = re.search(attendance_pattern, html_content)\n",
    "        attendance = int(attendance_match.group(1).replace(',', '')) if attendance_match else None\n",
    "        \n",
    "        # Extract game details (like shootout results)\n",
    "        details = None\n",
    "        for p in p_elements:\n",
    "            if \"shootout\" in p.text:\n",
    "                details = p.text\n",
    "                break\n",
    "        \n",
    "        # Clean details if present\n",
    "        if details and '\\n' in details:\n",
    "            details = details.replace('\\n', '').strip()\n",
    "        if details and '\\t' in details:\n",
    "            details = re.sub('\\t', ' ', details)\n",
    "        \n",
    "        game_details = {\n",
    "            'Day': day_of_week,\n",
    "            'Date': date,\n",
    "            'Conference': conference,\n",
    "            'Details': details,\n",
    "            'Location': location,\n",
    "            'Ref1': refs[0] if refs else None,\n",
    "            'Ref2': refs[1] if len(refs) > 1 else None,\n",
    "            'Asst_Ref1': asst_refs[0] if asst_refs else None,\n",
    "            'Asst_Ref2': asst_refs[1] if len(asst_refs) > 1 else None,\n",
    "            'Attendance': attendance\n",
    "        }\n",
    "        \n",
    "        game_details_df = pd.DataFrame([game_details])\n",
    "        return game_details_df\n",
    "\n",
    "    except (AttributeError, IndexError, ValueError) as e:\n",
    "        logging.error(f\"Error while parsing game details: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_box_score(box_score_html):\n",
    "    # Initialize DataFrames to None\n",
    "    scoring_summary = penalty_summary = goalie_stats = player_stats = line_chart = linescore = game_details = None\n",
    "    \n",
    "    try:\n",
    "        scoring_summary = parse_scoring_summary(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_scoring_summary: {e}\")\n",
    "    \n",
    "    try:\n",
    "        penalty_summary = parse_penalty_summary(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_penalty_summary: {e}\")\n",
    "    \n",
    "    try:\n",
    "        goalie_stats = parse_goalie_stats(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_goalie_stats: {e}\")\n",
    "    \n",
    "    try:\n",
    "        player_stats = parse_player_summary(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_player_summary: {e}\")\n",
    "    \n",
    "    try:\n",
    "        line_chart = parse_line_chart(box_score_html)\n",
    "        if line_chart.empty:\n",
    "            logging.info(\"Line chart is empty. Skipping the insert for this game.\")\n",
    "        else:\n",
    "            logging.info(f\"Line chart DataFrame structure: {line_chart.dtypes}\")\n",
    "\n",
    "        # Insert into database (make sure this part works as expected)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in parse_line_chart: {e}\")\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        linescore = parse_linescore(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_linescore: {e}\")\n",
    "    \n",
    "    try:\n",
    "        game_details = parse_game_details(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_game_details: {e}\")\n",
    "    \n",
    "    # Combine DataFrames into a list\n",
    "    all_dfs = [game_details, scoring_summary, penalty_summary, goalie_stats, player_stats, line_chart, linescore]\n",
    "    \n",
    "    return all_dfs\n",
    "\n",
    "\n",
    "def rename_duplicate_columns(df):\n",
    "    cols = pd.Series(df.columns)\n",
    "    for dup in df.columns[df.columns.duplicated()].unique(): \n",
    "        cols[df.columns.get_loc(dup)] = [f\"{dup}_{i}\" if i != 0 else dup for i in range(df.columns.get_loc(dup).sum())]\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to save DataFrames to SQLite database\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Modified Function to save DataFrames to SQLite database\n",
    "def save_to_sqlite_db(df_list, table_names, tag=''):\n",
    "    db_name = f\"../TEMP/{FILE_TAG}_Game_Stats.db\"\n",
    "    engine = create_engine(f'sqlite:///{db_name}')\n",
    "    \n",
    "    for df, table in zip(df_list, table_names):\n",
    "        try:\n",
    "            df = rename_duplicate_columns(df)\n",
    "            df.to_sql(table, engine, if_exists='append', index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to table {table}: {e}\")\n",
    "\n",
    "# ############ OLDER FUNCTION ################\n",
    "# def save_to_sqlite_db(df_list, table_names, tag=''):\n",
    "#     db_name = f\"../TEMP/Season_Data_2020.db\"  # Create DB file name using the TAG\n",
    "#     engine = create_engine(f'sqlite:///{db_name}')\n",
    "    \n",
    "#     for df, table in zip(df_list, table_names):\n",
    "#         # Assuming rename_duplicate_columns is a function you've defined earlier\n",
    "#         df = rename_duplicate_columns(df)\n",
    "#         df.to_sql(table, engine, if_exists='append', index=False)\n",
    "\n",
    "# Function to fetch and save data\n",
    "# Modified Function to fetch and save data\n",
    "def fetch_and_save_data_to_db(box_score_url, advanced_metrics_url, tag=f'{tag}'):  # Added tag parameter\n",
    "    db_name = f\"{tag} CHN Data.db\"  # Create DB name using the TAG\n",
    "    # Fetch HTML content for box score\n",
    "    box_score_response = requests.get(box_score_url)\n",
    "    box_score_html = box_score_response.text\n",
    "    \n",
    "    # Fetch HTML content for advanced metrics\n",
    "    advanced_metrics_response = requests.get(advanced_metrics_url)\n",
    "    advanced_metrics_html = advanced_metrics_response.text\n",
    "    \n",
    "    # Parse box score into list of DataFrames\n",
    "    box_score_dfs = parse_box_score(box_score_html)\n",
    "    \n",
    "    # Parse advanced metrics into list of DataFrames\n",
    "    advanced_metrics_df = parse_advanced_metrics_tables(advanced_metrics_html)\n",
    "    advanced_metrics_dfs = [advanced_metrics_df]\n",
    "    \n",
    "    # Combine all DataFrames into a list\n",
    "    all_dfs = box_score_dfs + advanced_metrics_dfs\n",
    "    \n",
    "    # Define table names for these DataFrames\n",
    "    table_names = ['game_details', 'scoring_summary', 'penalty_summary', \n",
    "                    'goalie_stats', 'player_stats', 'line_chart', 'linescore',\n",
    "                    'advanced_metrics']\n",
    "\n",
    "\n",
    "    # if len(all_dfs) != len(table_names):\n",
    "    #     raise ValueError(\"Mismatch between number of DataFrames and table names!\")\n",
    "\n",
    "    # Diagnostic Step 1: Print the number of DataFrames and table names\n",
    "    # print(f\"Number of DataFrames: {len(all_dfs)}\")\n",
    "    # print(f\"Number of table names: {len(table_names)}\")\n",
    "\n",
    "    # Diagnostic Step 2: Print the names of the columns for each DataFrame\n",
    "    # for df in all_dfs:\n",
    "    #     print(df.columns.tolist())\n",
    "\n",
    "\n",
    "    # Create a game_id for the game and apply it to all dataframes\n",
    "    # Game ID YYYMMDD-HomeTeam-AwayTeam\n",
    "    for df in all_dfs:\n",
    "        df['Game_ID'] = game_id\n",
    "\n",
    "        # Diagnostic check\n",
    "    if len(all_dfs) != len(table_names):\n",
    "        print(f\"Mismatch detected!\")\n",
    "        print(f\"box_score_url: {box_score_url}\")\n",
    "        print(f\"advanced_metrics_url: {advanced_metrics_url}\")\n",
    "    \n",
    "    # Save DataFrames to SQLite database\n",
    "    save_to_sqlite_db(all_dfs, table_names, tag)  # Pass the tag here\n",
    "    \n",
    "    return all_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# TAG = \"NEW2022-2023\"\n",
    "# save_to_sqlite_db(df_list, table_names, TAG)\n",
    "# fetch_and_save_data_to_db(box_score_url, advanced_metrics_url, TAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the GAME IDS from the results tab to the current databasefile and filter out games that there are already records for to avoid scraping the 4e3ntire season worth of games every time through the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Path to current database\n",
    "\n",
    "# current_db_path = f\"../TEMP/{FILE_TAG}_Game_Stats.db\"\n",
    "\n",
    "# # Create a connection to the database\n",
    "# # conn = sqlite3.connect(current_db_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop and Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping games: 100%|██████████| 1049/1049 [34:55<00:00,  2.00s/it]\n"
     ]
    }
   ],
   "source": [
    "## Run the scrape to get game data using the functions above and infor from games_df\n",
    "\n",
    "## Change the variable name to reuse the old code\n",
    "sampled_games = games_df\n",
    "\n",
    "# Initialize counters & logs\n",
    "error_count = 0\n",
    "error_games = []\n",
    "\n",
    "# Loop over sampled games and fetch data\n",
    "for idx, row in tqdm(sampled_games.iterrows(), total=sampled_games.shape[0], desc=\"Scraping games\"):\n",
    "    retries = 3  # Number of retries\n",
    "    success = False\n",
    "\n",
    "    \n",
    "    while retries > 0 and not success:\n",
    "        try:\n",
    "            box_score_url = base_url + row['Box_Link']\n",
    "            advanced_metrics_url = base_url + row['Metrics_Link']\n",
    "\n",
    "            # create a unique game id\n",
    "            game_id = str(row['Date']) + '-' + str(row['Home_Team']) + '-' + str(row['Away_Team'])\n",
    "            \n",
    "            logging.info(f\"Fetching data for game: {row['Home_Team']} vs {row['Away_Team']}\")\n",
    "            \n",
    "            # Your existing function to fetch and save data\n",
    "            all_dfs = fetch_and_save_data_to_db(box_score_url, advanced_metrics_url)\n",
    "            \n",
    "            # If reached here, the fetching was successful\n",
    "            success = True\n",
    "            \n",
    "            # Adaptive rate limiting\n",
    "            \n",
    "            \n",
    "        except requests.exceptions.RequestException as e:  # Network-related errors\n",
    "            logging.error(f\"Network error for game: {row['Home_Team']} vs {row['Away_Team']}. Error: {e}\")\n",
    "            retries -= 1\n",
    "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
    "        \n",
    "        except Exception as e:  # Other exceptions\n",
    "            logging.error(f\"An error occurred for game: {row['Game_ID']} - {row['Home_Team']} vs {row['Away_Team']}. Error: {e}\")\n",
    "            error_count += 1\n",
    "            error_games.append((row['Home_Team'], row['Away_Team']))\n",
    "            break  # Break the while loop; no retries for these types of errors\n",
    "\n",
    "\n",
    "# Close the logging file\n",
    "logging.shutdown()\n",
    "\n",
    "# Close the database connection\n",
    "# conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total games: 1049\n",
      "Games With Errors: 43\n",
      "Error: ('Arizona', 'Arizona State')\n",
      "Error: ('McGill', 'Vermont')\n",
      "Error: ('Merrimack', 'Sacred Heart')\n",
      "Error: ('Union', 'Rensselaer')\n",
      "Error: ('Royal Military', 'Niagara')\n",
      "Error: ('New Hampshire', 'Maine')\n",
      "Error: ('Western Michigan', 'US Under 18')\n",
      "Error: ('Manitoba', 'North Dakota')\n",
      "Error: ('Omaha', 'Minnesota State')\n",
      "Error: ('Simon Fraser', 'Colorado College')\n",
      "Error: ('Guelph', 'Rensselaer')\n",
      "Error: ('Canisius', 'Niagara')\n",
      "Error: ('Ottawa', 'Sacred Heart')\n",
      "Error: ('Quinnipiac', 'Northeastern')\n",
      "Error: ('Massachusetts', 'Dartmouth')\n",
      "Error: ('Bemidji State', 'Minnesota')\n",
      "Error: ('Guelph', 'RIT')\n",
      "Error: ('Toronto', 'Princeton')\n",
      "Error: ('US Under 18', 'Boston University')\n",
      "Error: ('TMU', 'Cornell')\n",
      "Error: ('Grand Valley St.', 'Ferris State')\n",
      "Error: ('Simon Fraser', 'Lake Superior')\n",
      "Error: ('US Under 18', 'Cornell')\n",
      "Error: ('St. Anselm', 'Long Island')\n",
      "Error: ('Simon Fraser', 'Robert Morris')\n",
      "Error: ('Anna Maria', 'Stonehill')\n",
      "Error: ('Simon Fraser', 'Long Island')\n",
      "Error: ('Long Island', 'St. Anselm')\n",
      "Error: ('US Under 18', 'St. Lawrence')\n",
      "Error: ('US Under 18', 'Clarkson')\n",
      "Error: ('Minot State', 'Colorado College')\n",
      "Error: ('Western Ontario', 'Western Michigan')\n",
      "Error: ('Ottawa', 'St. Lawrence')\n",
      "Error: ('US Under 18', 'North Dakota')\n",
      "Error: ('Minot State', 'Denver')\n",
      "Error: ('Simon Fraser', 'Boston University')\n",
      "Error: ('Minnesota Duluth', 'St. Thomas')\n",
      "Error: ('Simon Fraser', 'Boston College')\n",
      "Error: ('Simon Fraser', 'Providence')\n",
      "Error: ('Canisius', 'RIT')\n",
      "Error: ('Royal Military', 'Army')\n",
      "Error: ('Assumption', 'Stonehill')\n",
      "Error: ('Northern Michigan', 'Ferris State')\n",
      "Timestamp: 2024-02-25 14:04:12.527707\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "## Print a summary of the errors\n",
    "print(f\"Total games: {sampled_games.shape[0]}\")\n",
    "print(f\"Games With Errors: {error_count}\")\n",
    "for game in error_games:\n",
    "    print(f\"Error: {game}\")\n",
    "\n",
    "# Print Timestamp\n",
    "print(f\"Timestamp: {datetime.datetime.now()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix / Notes\n",
    "\n",
    "### Added 1/29/2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('player_stats_ytd',), ('master_roster',), ('game_details',), ('goalie_stats',), ('line_chart',), ('advanced_metrics',), ('linescore',), ('penalty_summary',), ('scoring_summary',), ('player_stats',)]\n"
     ]
    }
   ],
   "source": [
    "### Paths and File Names for the Data\n",
    "\n",
    "current_db_path = f\"../TEMP/{FILE_TAG}.db\"\n",
    "current_db_path = '../TEMP/FEB_22_Current_YTD_Stats_Game_Stats.db'\n",
    "\n",
    "conn = sqlite3.connect(current_db_path)\n",
    "\n",
    "# Roster data\n",
    "folder = '../data/rosters/'\n",
    "\n",
    "df_2023 = pd.read_csv(folder + '2023_master_roster.csv')\n",
    "df_2022 = pd.read_csv(folder + '2022_master_roster.csv')\n",
    "df_2021 = pd.read_csv(folder + '2021_master_roster.csv')\n",
    "df_2020 = pd.read_csv(folder + '2020_master_roster.csv')\n",
    "\n",
    "\n",
    "# Load the Correct Roster into the database\n",
    "roster_df = df_2023.copy()\n",
    "\n",
    "# Set the SeasonYear in the database_roster - Set to year the season started\n",
    "season_year_setting = 2023\n",
    "\n",
    "## Print tables in database\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cursor.fetchall())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary of Team Primary Names to Abbreviations\n",
    "\n",
    "- Needed to Add IVY teams becuase of low amount of games. will have to do for harvard, yale, ect next week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataframe from SQL query\n",
    "df = pd.read_sql_query(\"SELECT * FROM advanced_metrics\", conn)\n",
    "\n",
    "# Function to count the occurrences of primary team names for unmatched abbreviations\n",
    "def count_primary_names_for_abbreviation(abbreviation):\n",
    "    filtered_rows = df[df['Team'] == abbreviation]\n",
    "    team_counts = {}\n",
    "    \n",
    "    for _, row in filtered_rows.iterrows():\n",
    "        teams = row['Game_ID'].split('-')[-2:]\n",
    "        for team in teams:\n",
    "            if team not in team_counts:\n",
    "                team_counts[team] = 0\n",
    "            team_counts[team] += 1\n",
    "            \n",
    "    return team_counts\n",
    "\n",
    "\n",
    "# Attempt to match abbreviations to primary names based on substrings and common naming conventions\n",
    "matched_dict = {}\n",
    "unmatched_abbreviations = []\n",
    "\n",
    "for abbreviation in df['Team'].unique():\n",
    "\n",
    "\n",
    "\n",
    "# Match the abbreviation to the primary team name with the highest occurrence\n",
    "\n",
    "    team_counts = count_primary_names_for_abbreviation(abbreviation)\n",
    "    # Get the team with the highest count\n",
    "    matched_team = max(team_counts, key=team_counts.get)\n",
    "    matched_dict[abbreviation] = matched_team\n",
    "\n",
    "# matched_dict\n",
    "\n",
    "# Manually fix the unmatched abbreviations - IVY League Teams with no of very few games throw a wrench in the above method\n",
    "# Brown: Brown\n",
    "# Cornell: Cornell\n",
    "\n",
    "# Make those substitutions\n",
    "matched_dict['Brown'] = 'Brown'\n",
    "matched_dict['Cornell'] = 'Cornell'\n",
    "# yale\n",
    "matched_dict['Yale'] = 'Yale'\n",
    "# princeton\n",
    "matched_dict['Princeton'] = 'Princeton'\n",
    "\n",
    "# harvard\n",
    "matched_dict['Harvard'] = 'Harvard'\n",
    "# columbia\n",
    "matched_dict['Columbia'] = 'Columbia'\n",
    "\n",
    "# dartmouth\n",
    "matched_dict['Dartmouth'] = 'Dartmouth'\n",
    "\n",
    "# penn\n",
    "matched_dict['Penn'] = 'Penn'\n",
    "\n",
    "# BC\n",
    "matched_dict['BC'] = 'Boston College'\n",
    "\n",
    "\n",
    "\n",
    "# matched_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Transform Advanced Metrics\n",
    "- add, Team and Home-Away columns, combine the two tables into a single table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## NEW Handling of Advanced Stats\n",
    "# # Create dataframe from SQL query\n",
    "# df = pd.read_sql_query(\"SELECT * FROM advanced_metrics\", conn)\n",
    "\n",
    "# # Rename columns\n",
    "# new_names = ['Team', 'Player', 'TOTAL_Block', 'TOTAL_Miss', 'TOTAL_Saved', 'TOTAL_Goals', 'TOTAL_Total_Shots',\n",
    "#                 'EVEN_Block', 'EVEN_Miss', 'EVEN_Saved', 'EVEN_Goals', 'EVEN_Total_Shots',\n",
    "#                 'PP_Block', 'PP_Miss', 'PP_Saved', 'PP_Goals', 'PP_Total_Shots',\n",
    "#                 'CLOSE_Block', 'CLOSE_Miss', 'CLOSE_Saved', 'CLOSE_Goals', 'CLOSE_Total_Shots',\n",
    "\n",
    "#                 'D_Blocks', 'Faceoffs', 'Game_ID']\n",
    "\n",
    "# df.columns = new_names\n",
    "\n",
    "# # # Apply the matched_dict to the Team column\n",
    "# df['Team'] = df['Team'].apply(lambda x: matched_dict[x])\n",
    "\n",
    "# ## Fill all NaN values with 0\n",
    "# df = df.fillna(0)\n",
    "\n",
    "# # Display the dataframe\n",
    "# df.head()\n",
    "\n",
    "# # Save back to the database\n",
    "# df.to_sql('advanced_metrics', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Home and Away Columns to game_details table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Read the game_details table into a DataFrame\n",
    "# df_game_details = pd.read_sql(\"SELECT * FROM game_details\", conn)\n",
    "\n",
    "# # Step 2: Create new columns for Home and Away Teams by parsing Game_ID\n",
    "# df_game_details['Away_Team'] = df_game_details['Game_ID'].apply(lambda x: x.split('-')[3])\n",
    "# df_game_details['Home_Team'] = df_game_details['Game_ID'].apply(lambda x: x.split('-')[4])\n",
    "\n",
    "# # Step 3: Write this updated DataFrame back to the game_details table\n",
    "# df_game_details.to_sql('game_details', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Home_Team and Away_Team Columns to all tables\n",
    "### Use the Game_ID column to determin home and away, add to all tables\n",
    "- This will make infering stuff like Home Goals, away goals, ect much easier to work out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define function to add Home and Away Columns to tables\n",
    "\n",
    "def add_team_columns_to_tables(conn):\n",
    "    # Function to add team columns to a DataFrame\n",
    "    def add_team_columns(df):\n",
    "        df['Away_Team'] = df['Game_ID'].apply(lambda x: x.split('-')[3])\n",
    "        df['Home_Team'] = df['Game_ID'].apply(lambda x: x.split('-')[4])\n",
    "        return df\n",
    "\n",
    "    # Get the list of tables\n",
    "    tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    table_names = [t[0] for t in conn.execute(tables_query).fetchall()]\n",
    "\n",
    "    # Process each table\n",
    "    for table in table_names:\n",
    "        # Read the table into a DataFrame\n",
    "        df = pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
    "\n",
    "        # Check if the table contains the Game_ID column\n",
    "        if 'Game_ID' in df.columns:\n",
    "            # Add team columns\n",
    "            df = add_team_columns(df)\n",
    "\n",
    "            # Write the updated DataFrame back to the table\n",
    "            df.to_sql(table, conn, if_exists='replace', index=False)\n",
    "\n",
    "## Run The function on the open DB\n",
    "\n",
    "add_team_columns_to_tables(conn)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Clean up The Column Names and extra header rows in the Player Stats table\n",
    "# ############ 'Pt.' should be 'Pts' and '+/-' should be 'plus_minus'\n",
    "# #################################\n",
    "\n",
    "# # Define a dictionary for column renaming\n",
    "# column_renames = {\n",
    "#     'Pt.': 'Pts',\n",
    "#     '+/-': 'plus_minus'\n",
    "# }\n",
    "\n",
    "# # Rename columns based on the dictionary\n",
    "# player_stats_df.rename(columns=column_renames, inplace=True)\n",
    "\n",
    "\n",
    "# # Drop rows where Team name is in the Player column\n",
    "# player_stats_df = player_stats_df[player_stats_df['Team'] != player_stats_df['Player']]\n",
    "\n",
    "# # Print the length of the dataframe\n",
    "# len(player_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Pt.' not found.\n",
      "Column '+/-' not found.\n",
      "37822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37822"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Change the Column names to be easy to work with\n",
    "############ 'Pt.' should be 'Pts' and '+/-' should be 'plus_minus'\n",
    "#################################\n",
    "player_stats_df = pd.read_sql_query(\"SELECT * FROM player_stats\", conn)\n",
    "\n",
    "if 'Pt.' in player_stats_df.columns:\n",
    "    player_stats_df.rename(columns={'Pt.': 'Pts'}, inplace=True)\n",
    "else:\n",
    "    print(\"Column 'Pt.' not found.\")\n",
    "\n",
    "if '+/-' in player_stats_df.columns:\n",
    "    player_stats_df.rename(columns={'+/-': 'plus_minus'}, inplace=True)\n",
    "else:\n",
    "    print(\"Column '+/-' not found.\")\n",
    "\n",
    "print(len(player_stats_df))\n",
    "\n",
    "# Drop rows if Team name is in the player column\n",
    "# If ['Team'] is the same as ['Player'] then drop that row\n",
    "player_stats_df = player_stats_df[player_stats_df['Team'] != player_stats_df['Player']]\n",
    "\n",
    "# add the dataframe back to the database\n",
    "player_stats_df.to_sql('player_stats', conn, if_exists='replace', index=False)\n",
    "\n",
    "# print(len(player_stats_df))\n",
    "#################################\n",
    "# player_stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "      <th>goals1</th>\n",
       "      <th>goals2</th>\n",
       "      <th>goals3</th>\n",
       "      <th>goalsT</th>\n",
       "      <th>shots1</th>\n",
       "      <th>shots2</th>\n",
       "      <th>shots3</th>\n",
       "      <th>shotsT</th>\n",
       "      <th>Pen</th>\n",
       "      <th>...</th>\n",
       "      <th>PPG</th>\n",
       "      <th>PPO</th>\n",
       "      <th>FOW</th>\n",
       "      <th>FOL</th>\n",
       "      <th>FOW%</th>\n",
       "      <th>goals4</th>\n",
       "      <th>shots4</th>\n",
       "      <th>Game_ID</th>\n",
       "      <th>Away_Team</th>\n",
       "      <th>Home_Team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lake Superior</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-10-07-Lake Superior-Michigan State</td>\n",
       "      <td>Lake Superior</td>\n",
       "      <td>Michigan State</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michigan State</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>20</td>\n",
       "      <td>69.230769</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-10-07-Lake Superior-Michigan State</td>\n",
       "      <td>Lake Superior</td>\n",
       "      <td>Michigan State</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clarkson</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>48.275862</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-10-07-Clarkson-Notre Dame</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>Notre Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notre Dame</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>51.724138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-10-07-Clarkson-Notre Dame</td>\n",
       "      <td>Clarkson</td>\n",
       "      <td>Notre Dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RIT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>44.117647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-10-07-RIT-St. Lawrence</td>\n",
       "      <td>RIT</td>\n",
       "      <td>St. Lawrence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Team  goals1  goals2  goals3  goalsT  shots1  shots2  shots3  \\\n",
       "0   Lake Superior       0       2       0       2       8      11      12   \n",
       "1  Michigan State       1       2       2       5      13      16       8   \n",
       "2        Clarkson       1       0       2       3       8       6      11   \n",
       "3      Notre Dame       0       1       0       1       9       7      12   \n",
       "4             RIT       0       1       2       3       8      15       6   \n",
       "\n",
       "   shotsT  Pen  ...  PPG  PPO  FOW  FOL       FOW%  goals4  shots4  \\\n",
       "0      31    2  ...    0    2   20   45  30.769231       0       0   \n",
       "1      37    3  ...    1    2   45   20  69.230769       0       0   \n",
       "2      25    4  ...    0    3   28   30  48.275862       0       0   \n",
       "3      28    3  ...    0    4   30   28  51.724138       0       0   \n",
       "4      29    6  ...    0    3   30   38  44.117647       0       0   \n",
       "\n",
       "                                   Game_ID      Away_Team       Home_Team  \n",
       "0  2023-10-07-Lake Superior-Michigan State  Lake Superior  Michigan State  \n",
       "1  2023-10-07-Lake Superior-Michigan State  Lake Superior  Michigan State  \n",
       "2           2023-10-07-Clarkson-Notre Dame       Clarkson      Notre Dame  \n",
       "3           2023-10-07-Clarkson-Notre Dame       Clarkson      Notre Dame  \n",
       "4              2023-10-07-RIT-St. Lawrence            RIT    St. Lawrence  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Add The primary team names to the linescores table\n",
    "# Read the linescores table into a DataFrame\n",
    "df_linescores = pd.read_sql(\"SELECT * FROM linescore\", conn)\n",
    "\n",
    "# Apply the dictionary to the Team column\n",
    "# df_linescores['Team'] = df_linescores['Team'].apply(lambda x: matched_dict[x])\n",
    "\n",
    "df_linescores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Michigan State'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m df_penalty \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM penalty_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m, conn)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Apply the dictionary to the Team column\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#$ Skip if not found\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m df_penalty[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_penalty\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatched_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Apply same method to scorring_summary:\u001b[39;00m\n\u001b[0;32m     16\u001b[0m df_scoring \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM scoring_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m, conn)\n",
      "File \u001b[1;32mc:\\Users\\jbanc\\anaconda3\\envs\\data_viz\\Lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jbanc\\anaconda3\\envs\\data_viz\\Lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jbanc\\anaconda3\\envs\\data_viz\\Lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\jbanc\\anaconda3\\envs\\data_viz\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jbanc\\anaconda3\\envs\\data_viz\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[34], line 11\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      6\u001b[0m df_penalty \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM penalty_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m, conn)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Apply the dictionary to the Team column\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#$ Skip if not found\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m df_penalty[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_penalty[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeam\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mmatched_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Apply same method to scorring_summary:\u001b[39;00m\n\u001b[0;32m     16\u001b[0m df_scoring \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM scoring_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m, conn)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Michigan State'"
     ]
    }
   ],
   "source": [
    "# # Penalty Table & Scoring Summary\n",
    "\n",
    "# ## Add The primary team names to the linescores table\n",
    "# # Read the linescores table into a DataFrame\n",
    "\n",
    "# df_penalty = pd.read_sql(\"SELECT * FROM penalty_summary\", conn)\n",
    "\n",
    "# # Apply the dictionary to the Team column\n",
    "# #$ Skip if not found\n",
    "\n",
    "# df_penalty['Team'] = df_penalty['Team'].apply(lambda x: matched_dict[x])\n",
    "\n",
    "    \n",
    "\n",
    "# # Apply same method to scorring_summary:\n",
    "# df_scoring = pd.read_sql(\"SELECT * FROM scoring_summary\", conn)\n",
    "# df_scoring['Team'] = df_scoring['Team'].apply(lambda x: matched_dict[x])\n",
    "\n",
    "\n",
    "# ## Add each table back to database\n",
    "# # Write the updated linescores DataFrame back to the linescore table\n",
    "# df_linescores.to_sql('linescore', conn, if_exists='replace', index=False)\n",
    "\n",
    "# # Write the updated penalty DataFrame back to the penalty_summary table\n",
    "# df_penalty.to_sql('penalty_summary', conn, if_exists='replace', index=False)\n",
    "\n",
    "# # Write the updated scoring DataFrame back to the scoring_summary table\n",
    "# df_scoring.to_sql('scoring_summary', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clean_Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>G</th>\n",
       "      <th>A</th>\n",
       "      <th>Pts</th>\n",
       "      <th>plus_minus</th>\n",
       "      <th>Sh</th>\n",
       "      <th>PIM</th>\n",
       "      <th>Games_Played</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A.J. Hodges</td>\n",
       "      <td>Bentley</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>-2</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A.J. Macaulay</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AJ Casperson</td>\n",
       "      <td>Long Island</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Bohlinger</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aaron Grounds</td>\n",
       "      <td>Long Island</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-5</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Clean_Player           Team  G  A  Pts  plus_minus  Sh  PIM  \\\n",
       "0      A.J. Hodges        Bentley  6  7   13          -2  50    2   \n",
       "1    A.J. Macaulay         Alaska  2  9   11           2  34   14   \n",
       "2     AJ Casperson    Long Island  0  1    1           2   7    0   \n",
       "3  Aaron Bohlinger  Massachusetts  2  5    7           4  15    4   \n",
       "4    Aaron Grounds    Long Island  1  2    3          -5  14   16   \n",
       "\n",
       "   Games_Played  \n",
       "0            26  \n",
       "1            28  \n",
       "2            11  \n",
       "3            25  \n",
       "4            11  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CREATE A NEW TABLE WITH AGGRIGATED PLAYER STATS YEAR TO DATE\n",
    "\n",
    "# Use player_stats_df from here on, instead of running another SQL query.\n",
    "df_player_stats = player_stats_df.copy()\n",
    "\n",
    "\n",
    "# Clean up the name format in player_stats for easier matching\n",
    "# Replace the non-breaking space with a regular space\n",
    "df_player_stats['Clean_Player'] = df_player_stats['Player'].apply(lambda x: x.replace('\\xa0', ' '))\n",
    "\n",
    "# Remove rows where Player is the team name (e.g., \"Michigan State\")\n",
    "df_player_stats_cleaned = df_player_stats[df_player_stats['Player'] != df_player_stats['Team']]\n",
    "\n",
    "# Convert relevant columns to integers for correct aggregation\n",
    "cols_to_convert = ['G', 'A', 'Pts', 'plus_minus', 'Sh', 'PIM']\n",
    "for col in cols_to_convert:\n",
    "    df_player_stats_cleaned[col] = pd.to_numeric(df_player_stats_cleaned[col], errors='coerce')\n",
    "\n",
    "# Aggregate the data for year-to-date stats\n",
    "# Add a column for counting the number of games each player has played\n",
    "agg_player_stats_corrected_with_games = df_player_stats_cleaned.groupby(['Clean_Player', 'Team']).agg({\n",
    "    'G': 'sum',\n",
    "    'A': 'sum',\n",
    "    'Pts': 'sum',\n",
    "    'plus_minus': 'sum',\n",
    "    'Sh': 'sum',\n",
    "    'PIM': 'sum',\n",
    "    'Game_ID': 'count'  # Counting the number of unique Game_IDs for each player\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the Game_ID column to Games_Played\n",
    "agg_player_stats_corrected_with_games.rename(columns={'Game_ID': 'Games_Played'}, inplace=True)\n",
    "\n",
    "# Save the updated aggregated data back to the database, replacing the existing table\n",
    "agg_player_stats_corrected_with_games.to_sql('player_stats_ytd', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Verify by loading some sample data from the updated table\n",
    "sample_updated_ytd = pd.read_sql_query(\"SELECT * FROM player_stats_ytd LIMIT 5;\", conn)\n",
    "sample_updated_ytd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the Roster data from the CSVs to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['game_details',\n",
       " 'goalie_stats',\n",
       " 'line_chart',\n",
       " 'advanced_metrics',\n",
       " 'linescore',\n",
       " 'penalty_summary',\n",
       " 'scoring_summary',\n",
       " 'player_stats',\n",
       " 'player_stats_ytd',\n",
       " 'master_roster']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################## SET THE ROSTER DATAFRAME TO THE CORRECT YEAR ####################\n",
    "## MATCH THE DATAFRAME NAMES\n",
    "df_master_roster = roster_df.copy()\n",
    "\n",
    "## Season Year Value\n",
    "season_year = season_year_setting\n",
    "\n",
    "# Clean up the name formats for joining\n",
    "# Master roster: Convert \"Last Name, First Name\" to \"First Name Last Name\"\n",
    "# df_master_roster['Clean_Name'] = df_master_roster['Player'].apply(lambda x: ' '.join(reversed(x.split(', '))))\n",
    "\n",
    "# Rename Player to Clean_Name\n",
    "df_master_roster.rename(columns={'Player': 'Clean_Name'}, inplace=True)\n",
    "# Rename School to Team\n",
    "df_master_roster.rename(columns={'School': 'Team'}, inplace=True)\n",
    "\n",
    "# Clean up the Team column, remove '-' and replace with ' '\n",
    "# df_master_roster['School'] = df_master_roster['Team'].apply(lambda x: x.replace('-', ' '))\n",
    "\n",
    "## If there are an period in the column names, remove them\n",
    "df_master_roster.columns = df_master_roster.columns.str.replace('.', '')\n",
    "\n",
    "### Finally add the roster to the database as it's own table\n",
    "\n",
    "df_master_roster['SeasonYear'] = season_year\n",
    "\n",
    "# Save the roster data as a new table in the database\n",
    "roster_table_name = 'master_roster'\n",
    "df_master_roster.to_sql(roster_table_name, conn, if_exists='replace', index=False)\n",
    "############################################################\n",
    "\n",
    "# Verify by listing all the tables in the database again\n",
    "tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = conn.execute(tables_query).fetchall()\n",
    "table_names_updated = [table[0] for table in tables]\n",
    "table_names_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Close connection to database\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
